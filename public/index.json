[{"body":"","link":"http://localhost:1313/","section":"","tags":null,"title":""},{"body":"","link":"http://localhost:1313/post/","section":"post","tags":null,"title":"Posts"},{"body":"Platform engineering should follow similar patterns as restaurants, yet we often confuse it with a kitchen or a supermarket.\nLet me explain.\nSome people like to cook. Few are actually skilled at cooking. Majority of us are skilled at something else and prefer a meal in a restaurant rather than cooking it ourselves, especially if that means putting a frozen pizza into a microwave.\nFor us not skilled in art of preparing meals, cooking is not something we're looking forward to. Getting ingredients is a chore. We need to know what to buy, where to buy it, how much to buy, how to store it, how to prepare it, how to cook it, and how to serve it. We need to know what to do if something goes wrong. We need to know how much it costs, how long it takes, and when it will be ready.\nI, for example, do not like cooking. I like writing code. I'm almost certainly better at writing code than cooking.\nSometimes I might combine both. Sometime I go to a restaurant, order a meal, and write code while waiting for it. I do what I do best, and someone else, in this case a cook, does what they do best.\nPlatform engineering is like a restaurant. It's a set of services we offer to developers so that they can do what they need to do while maintaining the focus on their core business, on writing code.\nThat being said, there are certain rules in most of the restaurants. Those rules provide great experience to guests while, at the same time, enabling restaurant stuff to do their jobs. The major difference is that, in case of platform engineering, we might not need staff.\nHere are the rules of a restaurant that we can apply to platform engineering:\nTo begin with, we do NOT order ingredients in a restaurant but meals. We can order a burger, or pasta, or pizza, or a steak. We do not order a caw, flour, and tomatoes.\nThe same can be said for platform engineering. We should not offer \u0026quot;ingredients\u0026quot; like networking, storage, or compute, but, rather a database server with databases and schemas, a highly available application, a production-ready cluster, and so on and so forth. We are offering solutions, rather than pieces of a puzzle that developers need to put together. We are offering meals, rather than ingredients.\nUnless I am a cook or a networking expert, database administrator, or a security officer, I do not want to deal with ingredients. If I am a developer, I expect to consume everything my code might need as a ready-to-go solution.\nNext, meals are ordered through a menu. The restaurant prepared a number of dishes that we can order. It cannot be anything we want. We cannot go into a restaurant and ask for a grilled snake with a side of fried grasshoppers, at least not in restaurants I visit. A restaurant can change the menu over time. It can add additional meals, it can improve on the existing meals, and it can remove the meals that are not popular. That does not mean that we cannot have a meal that is not in the menu. We can, but not necessarily in that restaurant. We are free to go home and cook our own meal any way we want. We can even open our own restaurant. But, in a specific restaurant, the menu is the catalog of what is available.\nThe same goes with platform engineering. We offer solutions that are decided in advance. We improve them over time, we add new ones, and we remove the ones that are not commonly used. Just as we are free to cook our own meals at home, developers are free to build their own solutions. They can even open their own platform engineering department. But, in a specific platform, the menu is the catalog of what is available. We call that a service catalog. It contains the list of solutions that are available to developers. It's not a list of ingredients, it's a list of meals.\nThe protocol for ordering a meal is simple. A waiter gives us the menu, we choose a meal, and we tell the waited what we chose. The waiter does not prepare meals. That's done by cooks in the kitchen. The waiter is the interface between us the the kitchen stuff. We cannot go to the kitchen to take a peak and we cannot interact with cooks. We do not deal with dirty dishes, we do not clean the kitchen floor, and we do no cut the vegetables. Again, we are free to do all that at home, but not in a restaurant.\nIn platform engineering, the protocol we use to communicate with the system that provides solutions is API. It can be HTTP, gRPC, or anything else. What it cannot or, to be more precise, it should not be are scripts, random manifests, HCL or Helm files, or anything else that is not an API. Now, just as a waiter can be dressed casually or in a uniform or can speak with american or french access, the way we access those APIs can be direct or indirect through a CLI, a web interface, or a chatbot. But, in the end, it's an API. Not building APIs is like not having waiters in a restaurant and expecting guests to deal with the same stuff as cooks or expecting cooks to read your mind. We adopted AWS, Google Cloud, Azure, and other hyperscalers mostly because they expose everything through their APIs. Just as you would not accept any of those if they would be a bunch of scripts, your customers, developers in your company, should not accept it either. Our customers, developers, are free to wrap those APIs into scripts, manifests, HCL, Helm files, or anything else they want. But, in the end, it's all about APIs. Heck, we might even help them create those scripts, manifests, HCL, or Helm files. Still, it's all about APIs. That's the solution, everything else is a duct tape.\nAPIs are also one of the critical differences between restaurants and platform engineering. There are no humans involved in APIs. We are involved in designing them and making sure they are running, but we are not involved in fullfilling requests. A user wants something, that something is communicated to the \u0026quot;kitchen\u0026quot; through an API, and that's it. If your API is a ticketing system, you're doing it wrong. You can think of the API as a robot waiter. It works tirelesly at accepting your requests and propagating them to the system that creates whatever we need to be created.\nArguable, the critical components in a restaurant are cooks. It does not matter how well written menus are or how well behaved waiters are, if cooks are not good at what they do. We order something, and shortly afterwards that something is brought to us by a waiter. Behind the scenes, the waiter delivered the order to a cook who prepared that order and gave it to the waiter. The waiter then brought it to us. If we would have closed our eyes long enough, it would look like a meal materialized out of thin air. But it's not only that. It's not only about meals materializing in front of us, but those meals being delicious and what we expect them to be.\nSimilarly, platform engineering is also all about creating services that can be consumed by others. But, just as waiters are replaced by APIs, cooks are replaced by services. If we request, through an API, a database server with a few databases and a schema, there is no person on the other end that spins up a server, sets up a database server, creates a few databases, and applies a schema. It's all happening automagically through services. Just as when we request an EC2 instance from AWS, there are no gremlins in their data centers that are running around and setting up a server for us. It's all automated. Our job, as platform engineers is to create a system on the other end of the API that will do all that for us. Our job is to design and maintain services, rather than to spin up instances of those services.\nThe next rule is that restaurants need to offer some level of flexibility with the menu. We might have alergies that prevent us from eating certain ingredients or we might have taste slightly different from what the restaurant offers. I, for example, do not eat brocoli and if the dish I want contains it, I might ask for it to be removed. If i order a stake, waiter is likely going to ask me whether I want is rare, medium, or well done. Now, not everything is cosutomizable. Plates presented in a menu can be tweaked, but only up to a point. A friend of mine is vegan and she will not order a stake and ask for it to be made out of tofu. She will order something that is already vegan or close to be vegan. She might order a pizza and ask for cheese to be removed.\nHence, restaurants are flexible, but not too flexible. They offer a set of meals that can be customized to some extent. They do not offer a set of ingredients that can be combined in any way we want. That would be a supermarket. Ultimately, it's up to us to choose the type of a restaurant. When I eat alone, I tend to go to a stake house but when I'm accompanied by my vegan friend, we tend to find a restaurant with the menu that suits both of us. We do not go to a vegan restaurant since I am never going to get a staake there.\nServices offered through an internal platform follow the same logic. Services do what they are supposed to do. Database-as-a-Service cannot be transformed to become a Kubernetes cluster. If that's what someone needs and it is not offered in the catalog, that someone needs to set it up themselves. But, if a service is close to what someone needs, they can ask for it to be customized. Just as a stake can be rare, medium, or well done, a database server can be small, medium, or large. Just as a stake can be served with a side of fries or a salad, a database server can be served with a few databases and schemas. Just as a stake can be served with a sauce, a database server can be served with a backup and a monitoring. Just as a stake can be served with a glass of wine, a database server can be served with a few read replicas. But, just as a stake cannot be transformed into a pizza, a database server cannot be transformed into a Kubernetes cluster.\nThink of servies as being robots assembling lego blocks. Blocks are what they are but the end result can be different depending on instructions it received. The end solution might differ, but not too much. It cannot end up building a table from IKEA using lego blocks.\nMenus need to have information. The most obvious one is the price. We want to know how much a meal costs so that we can make a decision whether we want to order it or not. We might expect to have an information whether it contains gluten or nuts. Now, that information is not supposed to be too detailed. We do not expect to see recipes in a menu.\nThat's what grandmas are for. You can ask them how to mix ingredients to get the meal you remember from your childhood. Do not try to enter restaurant kitchen inquiring about the recipe.\nService catalogs should follow the same logic. We might want to know the price of a service. We might want to know whether a database is highly available or not, whether it is PostgreSQL or MySQL, whether it is encrypted or not, whether it is backed up or not, and so on and so forth. We do not expect to see a detailed description of how the service is implemented. That's what documentation is for. You can read it if you want to know how the service is implemented. Most people do not care about that. They care about what the service does, how much it costs, and a few other details. Not providing any information is bad but providing too much information is also bad.\nWhen we order, a waiter will tells us that our order will be ready soon. We will be informed if, for some reason, our order is delayed. We get feedback. When it does arrive, we might inspect it.\nIf we ordered a bottle of wine, we will be given an opportunity to taste it before we commit to it. When the stake arrives, we will be able to see whether it was done to our liking. If it's too rare for our taste, we have the opportunity to send it back to be cooked a bit more. We can even update our order by ordering fries which we forgot to ask for initially. Now, we do not get a microscope to inspect the stake and we do not get a chemistry set to test the wine. We get just enough information to evaluate the meal and, if needed, to update our order.\nIn platform engineering, those examples are equivalent to observability. From the moment we request an instance of a service we should be able to observe the status, to see logs, to query metrics, and to see traces. We should be able to see real time cost as opposed to the estimated cost, when it will be ready, and whether something went wrong. Now, the part that many get wrong is by providing all that information in the same form as the information observed by service owners, the people that observe the whole system and people who manage those services. That is equivalent to getting a microscope to inspect the stake. We do not need that. We need observability data filtered and transformed into a format that is appropriate to service consumers, not service owners. Consumers are... well, consumers. They are not database administrators. If they were, they would be creating Database-as-a-Service, not consuming it. They are not Kubernetes experts, security officers, or network engineers. They are developers. They are people who write code and need that code running or talking to a database or... you get the point.\nAll in all, platform engineering is like opening and managing a restaurant. We build services for different types of needs developers need. That can be a way to create and manage databases, Kubernetes clusters, applications, or anything else . What those solutions are depends on the needs of our customers, of developers.\nSolutions are offered in form of a catalog that contains the list of what can be instantiated . That catalog is equivalent to a menu.\nWe order instances of those solutions through an API . That API is equivalent to a waiter. It takes the order and sends it to the kitchen where meals are prepared. The menu, the service catalog, is a reflection of what can be prepared in the kitchen and, when done right, that catalog is auto-generated from APIs .\nOnce something is requested through the API, services behind that API does whatever needs to be done. That can be spinning up a server, setting up a database, creating a schema, and so on and so forth . That is equivalent to cooks in a restaurant.\nService instances can be customizable. Requests for service instances can be made without any additional information by simply requesting a database, or by customizing requests . API endpoints have schemas with default values. If no values are provided, defaults are used. Defaults can be overwritten with our custom values. That is equivalent to customizing a meal. Some data might not have default values and, in that case, service consumers are forced to provide additional information just as we have to specify whether we want a stake rare, medium, or well done.\nService catalogs provide the right level of information and hide the details that are not relevant to consumers. They might need to know the cost, the type of a database, whether it is encrypted, and so on and so forth. They might not need to know how the service is implemented, which VPC is used, what are the subnets, and majority of nitty-gritty details that matter to service providers, to you, but not to them. It is up to us to figure out how to filter relevant information and transform it into a format that is appropriate for service consumers.\nFinally, service consumers need to have observability. They need to know status of service instances, logs, metrics, and traces. They need to know when something goes wrong, when it will be ready, and how much it actually costs. That information is also filtered and transformed into a format that is appropriate for service consumers. We don't throw everything we have at them. We give them just enough information needed for them to have confidence in the system and understand how it affects their work .\nAll that is done without our direct involvement. We are not waitress, we are not cooks, we are not cleaning staff. We are designers of the restaurant, the kitchen, and the menu. We are the ones that make sure that everything is working properly. We are the ones that make sure that the restaurant is cleaned, that the kitchen is well equipped, and that the menu is up to date. We are the ones that make sure that the restaurant is running smoothly. Restaurant staff is automated. It consists of bits and bytes, not people.\nWe design and operate the API and what's on the right of it. We do not operate what's on the left. On the left are our users. They are the ones that order meals. They are the ones that consume those meals. They are the ones that pay for those meals. They are the ones that give us feedback. They are the ones that decide whether they will come back or not. They are the ones that decide whether they will recommend us to their colleagues. They are the ones that decide whether we will be successful or not.\nNevertheless, more often than not, we do not do those things. We do not design APIs and services but, rather, we give people a bunch of scripts and HCL files and Helm charts or we wrap those into some UIs. That would be similar to a waiter coming back to us with a bucket full of carrots, a chicken, and a bag of flour and saying \u0026quot;Here you go. I selected them for you and I already cut them, you do the rest.\u0026quot; That's not what we expect in a restaurant and that's not what we should expect in platform engineering.\n","link":"http://localhost:1313/post/platform-engineering-menu/","section":"post","tags":null,"title":"How Platform Engineering Compares to Running a Restaurant"},{"body":"I'm in pain... and it's self-inflicted... and I like it.\nI tend to go through an endless number of tools, services, and format in search for better ways to do my job. I'm never satisfied. I always think that there is something better out there. So I go through pain of learning a new tool or a language only to jump into a new one shortly afterwards. Spending endless hours going through new stuff does not make sense, but I can't help myself. Hopefully, I might save you from doing it yourself. That's my goal. Go through the pain of trying out everything so that you don't have to.\nLatelly, I've been exploring different ways to manage manifests, mostly for Kubernetes resources, but, in reality, for anything that can be defined as data, meaning YAML, JSON, XML, and so on.\nToday I want to explore yet another language designed to work with data structures and that language is... KCL. It is a constraint-based record and functional language and, as I'm pronouncing it, I realize that such a description might not make sense to most of use, so here's a simplified version.\nKCL is a language designed to work with data with the goal to produce YAMl or JSON. From that perspective it is similar to Helm, Kustomize, Jsonnet, Carvel ytt, Pkl, and, my current favorites, CUE.\nNow, you might be asking \u0026quot;why do we need another one of those?\u0026quot;, and that's what I've been asking myself and others as well. Nevertheless, I'm here to explore it and see if it makes sense to use it.\nNow, before we proceed, let me tell you what I'm looking for.\nI want to use a language or a DSL that is designed for data structures. The reason for that is that YAML or JSON I might be producing is data. That requirement alone discards Helm since Helm does not understand data. Helm is a free-text templating engine that has no notion of data. It is based on Go templating that is equally bad at genering HTML pages as generating data. Nevertheless, Helm is the de-facto standard for third-party apps so I have to use it for that, but not for my apps.\nThen there is Kustomize that is my favorite when simple scenarios are concerned. It's not a language but, rather, a mechanism that allows us to overlay YAML files with other YAML files. It is simple and effective, but it fails miserably for anything but very simple scenarios. When things are done right, scenarios are simple so I use it heavily, yet, there are cases when I need more.\nThen there is Jsonnet, Carvel ytt, and a myriad of other solutions which, for one reason or another, I gave up on. Currently, I'm torn between CUE with Timoni and Pkl. I explored all those so I won't go into them today. Check them out if you're not familiar with them.\nBoth of those are doing just what I need them to do, and that's where mazochism kicks in. I don't need another one, yet I'm exploring KCL. There are a few reasons for doing that besides enjoying self-inflicted pain.\nGoing back to my requirements...\nBesides the need for a language or a DSL that understands data-structures instead of being general-purpose anything goes, I also need to be able to define schemas but also to import schemas. It would be silly for me to reinvent the wheel by creating schemas for, let's say, Kubernetes APIs. I expect those to be readily available and, in case of CRDs, to have the option to import them from a cluster or a Git repo.\nI also insist on immutability. I experienced too many issues when working with mutable data. I want to avoid that at all costs.\nFinally, I don't want to spend weeks trying to learn it. A day is more than enough. If it takes more than that, excluding special \u0026quot;advanced\u0026quot; features, it's too complicated for my tiny brain. It needs to be easy.\nSo, I'm looking for a data-structure language that comes with pre-built schemas but also allows me to create them myself or import them from somewhere, I need it to be immutable and it cannot be hard to learn.\nWith that in mind, let's take a look at KCL and see whether it fits those requirements and whether it might convince me to drop at CUE or Pkl.\nLet's see what KCL is all about.\nSetup Install kcl CLI https://kcl-lang.io/docs/user_docs/getting-started/install#1-install-kcl\nInstall kcl Language Server and IDE extension from https://kcl-lang.io/docs/user_docs/getting-started/install#2-install-kcl-ide-extension.\n1git clone https://github.com/vfarcic/crossplane-app 2 3cd crossplane-app 4 5git pull 6 7git checkout kcl KCL in Action Before we begin, I must say that KCL is a CNCF project meaning that it is not in the hands of a single company making it's future depend less on the whims of that company. It generated a lot of buzz and it has a very active community. There must be something in there, right?\nRight now, I'm inside a project that requires a rather large amount of YAML. That YAML is big enough and with enough complexity and repetition that it makes no sense to write it directly. I need to generate it somehow, and I was about to switch to CUE when I got introduced to KCL so I though \u0026quot;What the heck. Let's give it a try.\u0026quot;\nAs you can expect, I can execute kcl with the path to the code, hit the enter key, and...\n1kcl kcl/backend.k The output is as follows.\n1apiVersion: apiextensions.crossplane.io/v1 2kind: Composition 3metadata: 4 name: app-backend 5 labels: 6 type: backend 7 location: local 8spec: 9 compositeTypeRef: 10 apiVersion: devopstoolkitseries.com/v1alpha1 11 kind: App 12 patchSets: 13 - name: metadata 14 patches: 15 - fromFieldPath: metadata.labels 16 resources: 17 - name: kubernetes 18 base: 19 apiVersion: kubernetes.crossplane.io/v1alpha1 20 kind: ProviderConfig 21 spec: 22 credentials: 23 source: InjectedIdentity 24 patches: 25 - fromFieldPath: spec.id 26 toFieldPath: metadata.name 27 readinessChecks: 28 - type: None 29 - name: deployment 30 base: 31 apiVersion: kubernetes.crossplane.io/v1alpha1 32 kind: Object 33 spec: 34 forProvider: 35 manifest: 36 apiVersion: apps/v1 37 kind: Deployment 38 spec: 39 selector: {} 40 template: 41 spec: 42 containers: 43 - livenessProbe: 44 httpGet: 45 path: / 46 port: 80 47 name: backend 48 ports: 49 - containerPort: 80 50 readinessProbe: 51 httpGet: 52 path: / 53 port: 80 54 resources: 55 limits: 56 cpu: 250m 57 memory: 256Mi 58 requests: 59 cpu: 125m 60 memory: 128Mi 61 patches: 62 - fromFieldPath: spec.id 63 toFieldPath: metadata.name 64 transforms: 65 - type: string 66 string: 67 fmt: \u0026#39;%s-deployment\u0026#39; 68 - fromFieldPath: spec.id 69 toFieldPath: spec.forProvider.manifest.metadata.name 70 - fromFieldPath: spec.parameters.namespace 71 toFieldPath: spec.forProvider.manifest.metadata.namespace 72 - fromFieldPath: spec.id 73 toFieldPath: spec.forProvider.manifest.metadata.labels.app 74 - fromFieldPath: spec.id 75 toFieldPath: spec.forProvider.manifest.spec.selector.matchLabels.app 76 - fromFieldPath: spec.id 77 toFieldPath: spec.forProvider.manifest.spec.template.metadata.labels.app 78 - fromFieldPath: spec.parameters.image 79 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].image 80 - fromFieldPath: spec.parameters.port 81 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].ports[0].containerPort 82 - fromFieldPath: spec.parameters.port 83 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].livenessProbe.httpGet.port 84 - fromFieldPath: spec.parameters.port 85 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].readinessProbe.httpGet.port 86 - fromFieldPath: spec.id 87 toFieldPath: spec.providerConfigRef.name 88 - name: service 89 base: 90 apiVersion: kubernetes.crossplane.io/v1alpha1 91 kind: Object 92 spec: 93 forProvider: 94 manifest: 95 apiVersion: v1 96 kind: Service 97 spec: 98 ports: 99 - name: http 100 port: 8008 101 protocol: TCP 102 type: ClusterIP 103 patches: 104 - fromFieldPath: spec.id 105 toFieldPath: metadata.name 106 transforms: 107 - type: string 108 string: 109 fmt: \u0026#39;%s-service\u0026#39; 110 - fromFieldPath: spec.id 111 toFieldPath: spec.forProvider.manifest.metadata.name 112 - fromFieldPath: spec.parameters.namespace 113 toFieldPath: spec.forProvider.manifest.metadata.namespace 114 - fromFieldPath: spec.id 115 toFieldPath: spec.forProvider.manifest.metadata.labels.app 116 - fromFieldPath: spec.id 117 toFieldPath: spec.forProvider.manifest.spec.selector.app 118 - fromFieldPath: spec.parameters.port 119 toFieldPath: spec.forProvider.manifest.spec.ports[0].port 120 - fromFieldPath: spec.parameters.port 121 toFieldPath: spec.forProvider.manifest.spec.ports[0].targetPort 122 - fromFieldPath: spec.id 123 toFieldPath: spec.providerConfigRef.name 124 - name: ingress 125 base: 126 apiVersion: kubernetes.crossplane.io/v1alpha1 127 kind: Object 128 spec: 129 forProvider: 130 manifest: 131 apiVersion: networking.k8s.io/v1 132 kind: Ingress 133 metadata: 134 annotations: 135 ingress.kubernetes.io/ssl-redirect: \u0026#39;false\u0026#39; 136 spec: 137 rules: 138 - http: 139 paths: 140 - backend: 141 service: 142 name: acme 143 path: / 144 pathType: ImplementationSpecific 145 patches: 146 - fromFieldPath: spec.id 147 toFieldPath: metadata.name 148 transforms: 149 - type: string 150 string: 151 fmt: \u0026#39;%s-ingress\u0026#39; 152 - fromFieldPath: spec.id 153 toFieldPath: spec.forProvider.manifest.metadata.name 154 - fromFieldPath: spec.parameters.namespace 155 toFieldPath: spec.forProvider.manifest.metadata.namespace 156 - fromFieldPath: spec.id 157 toFieldPath: spec.forProvider.manifest.metadata.labels.app 158 - fromFieldPath: spec.parameters.host 159 toFieldPath: spec.forProvider.manifest.spec.rules[0].host 160 - fromFieldPath: spec.id 161 toFieldPath: spec.forProvider.manifest.spec.rules[0].http.paths[0].backend.service.name 162 - fromFieldPath: spec.parameters.port 163 toFieldPath: spec.forProvider.manifest.spec.rules[0].http.paths[0].backend.service.port.number 164 - fromFieldPath: spec.id 165 toFieldPath: spec.providerConfigRef.name 166 - type: ToCompositeFieldPath 167 fromFieldPath: spec.forProvider.manifest.spec.rules[0].host 168 toFieldPath: status.host There we go.\nThe output is over 150 lines of YAML.\nNow that you saw the monstruosity that was generated, let's take a look at the KCL definition that made that possible.\n1cat kcl/backend.k The output is as follows.\n1import .common 2import .deployment 3import .service 4import .ingress 5import .kubernetesProviderConfig 6 7common.Composition { 8 metadata = common.Metadata { 9 name = \u0026#34;app-backend\u0026#34; 10 labels = common.Labels { 11 type = \u0026#34;backend\u0026#34; 12 location = \u0026#34;local\u0026#34; 13 } 14 } 15 spec = common.Spec { 16 resources = [ 17 kubernetesProviderConfig.KubernetesProviderConfig {} 18 deployment.Deployment {} 19 service.Service {} 20 ingress.Ingress {} 21 ] 22 } 23} At the very top, I am importing schemas, functions, global variables, and a few other things. There's common that contains, as you might expect from the name, common stuff that I could not place in a specific \u0026quot;box\u0026quot;. Then there are deployment, service, and ingress which are my abstractions that match corresponding Kubernetes resources, even though this YAML is generating Crossplane Compositions and not Kubernetes resources directly. Finally, there's kubernetesProviderConfig that is a Crossplane ProviderConfig.\nSuch an organization allows me to avoid repetition since, as you will see soon, those imports are repeated across other KCL manifests.\nThe actual output is generated by invoking Composition schema defined in common, we'll see it soon. That schema already contains all the values that are the same for all variations as well as variables that need to be defined explicitly. In this case, I have to define only the name, the labels, and the resources array that, in this case, contains deployment, service, and ingress schemas also imported at the top.\nHere's a variation of that manifest that generates a similar output but with a few differences.\n1cat kcl/backend-db-remote.k The output is as follows.\n1import .common 2import .deployment 3import .service 4import .ingress 5 6common.Composition { 7 metadata = common.Metadata { 8 name = \u0026#34;app-backend-db-remote\u0026#34; 9 labels = common.Labels { 10 type = \u0026#34;backend-db\u0026#34; 11 location = \u0026#34;remote\u0026#34; 12 } 13 } 14 spec = common.Spec { 15 resources = [ 16 deployment.Deployment{ 17 _dbEnabled = True 18 _dbSecretName = \u0026#34;spec.parameters.dbSecret.name\u0026#34; 19 _providerConfigName = \u0026#34;spec.parameters.kubernetesProviderConfigName\u0026#34; 20 }, 21 service.Service{ 22 _providerConfigName = \u0026#34;spec.parameters.kubernetesProviderConfigName\u0026#34; 23 }, 24 ingress.Ingress{ 25 _providerConfigName = \u0026#34;spec.parameters.kubernetesProviderConfigName\u0026#34; 26 }, 27 ] 28 } 29} This KCL definition will generate a different Composition. Besides a different name and labels, customized versions of the Deployment, Service, and Ingress by passing variables to their respective schemas. We'll see those soon. For now, let's take a look at the output of that KCL definition.\n1kcl kcl/backend-db-remote.k The output is as follows.\n1apiVersion: apiextensions.crossplane.io/v1 2kind: Composition 3metadata: 4 name: app-backend-db-remote 5 labels: 6 type: backend-db 7 location: remote 8spec: 9 compositeTypeRef: 10 apiVersion: devopstoolkitseries.com/v1alpha1 11 kind: App 12 patchSets: 13 - name: metadata 14 patches: 15 - fromFieldPath: metadata.labels 16 resources: 17 - name: deployment 18 base: 19 apiVersion: kubernetes.crossplane.io/v1alpha1 20 kind: Object 21 spec: 22 forProvider: 23 manifest: 24 apiVersion: apps/v1 25 kind: Deployment 26 spec: 27 selector: {} 28 template: 29 spec: 30 containers: 31 - env: 32 - name: DB_ENDPOINT 33 valueFrom: 34 secretKeyRef: 35 key: endpoint 36 - name: DB_PASSWORD 37 valueFrom: 38 secretKeyRef: 39 key: password 40 - name: DB_PORT 41 valueFrom: 42 secretKeyRef: 43 key: port 44 optional: true 45 - name: DB_USERNAME 46 valueFrom: 47 secretKeyRef: 48 key: username 49 - name: DB_NAME 50 livenessProbe: 51 httpGet: 52 path: / 53 port: 80 54 name: backend 55 ports: 56 - containerPort: 80 57 readinessProbe: 58 httpGet: 59 path: / 60 port: 80 61 resources: 62 limits: 63 cpu: 250m 64 memory: 256Mi 65 requests: 66 cpu: 125m 67 memory: 128Mi 68 patches: 69 - fromFieldPath: spec.id 70 toFieldPath: metadata.name 71 transforms: 72 - type: string 73 string: 74 fmt: \u0026#39;%s-deployment\u0026#39; 75 - fromFieldPath: spec.id 76 toFieldPath: spec.forProvider.manifest.metadata.name 77 - fromFieldPath: spec.parameters.namespace 78 toFieldPath: spec.forProvider.manifest.metadata.namespace 79 - fromFieldPath: spec.id 80 toFieldPath: spec.forProvider.manifest.metadata.labels.app 81 - fromFieldPath: spec.id 82 toFieldPath: spec.forProvider.manifest.spec.selector.matchLabels.app 83 - fromFieldPath: spec.id 84 toFieldPath: spec.forProvider.manifest.spec.template.metadata.labels.app 85 - fromFieldPath: spec.parameters.image 86 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].image 87 - fromFieldPath: spec.parameters.port 88 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].ports[0].containerPort 89 - fromFieldPath: spec.parameters.port 90 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].livenessProbe.httpGet.port 91 - fromFieldPath: spec.parameters.port 92 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].readinessProbe.httpGet.port 93 - fromFieldPath: spec.parameters.dbSecret.name 94 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[0].valueFrom.secretKeyRef.name 95 - fromFieldPath: spec.parameters.dbSecret.name 96 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[1].valueFrom.secretKeyRef.name 97 - fromFieldPath: spec.parameters.dbSecret.name 98 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[2].valueFrom.secretKeyRef.name 99 - fromFieldPath: spec.parameters.dbSecret.name 100 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[3].valueFrom.secretKeyRef.name 101 - fromFieldPath: spec.parameters.dbSecret.name 102 toFieldPath: spec.forProvider.manifest.spec.template.spec.containers[0].env[4].value 103 - fromFieldPath: spec.parameters.kubernetesProviderConfigName 104 toFieldPath: spec.providerConfigRef.name 105 - name: service 106 base: 107 apiVersion: kubernetes.crossplane.io/v1alpha1 108 kind: Object 109 spec: 110 forProvider: 111 manifest: 112 apiVersion: v1 113 kind: Service 114 spec: 115 ports: 116 - name: http 117 port: 8008 118 protocol: TCP 119 type: ClusterIP 120 patches: 121 - fromFieldPath: spec.id 122 toFieldPath: metadata.name 123 transforms: 124 - type: string 125 string: 126 fmt: \u0026#39;%s-service\u0026#39; 127 - fromFieldPath: spec.id 128 toFieldPath: spec.forProvider.manifest.metadata.name 129 - fromFieldPath: spec.parameters.namespace 130 toFieldPath: spec.forProvider.manifest.metadata.namespace 131 - fromFieldPath: spec.id 132 toFieldPath: spec.forProvider.manifest.metadata.labels.app 133 - fromFieldPath: spec.id 134 toFieldPath: spec.forProvider.manifest.spec.selector.app 135 - fromFieldPath: spec.parameters.port 136 toFieldPath: spec.forProvider.manifest.spec.ports[0].port 137 - fromFieldPath: spec.parameters.port 138 toFieldPath: spec.forProvider.manifest.spec.ports[0].targetPort 139 - fromFieldPath: spec.parameters.kubernetesProviderConfigName 140 toFieldPath: spec.providerConfigRef.name 141 - name: ingress 142 base: 143 apiVersion: kubernetes.crossplane.io/v1alpha1 144 kind: Object 145 spec: 146 forProvider: 147 manifest: 148 apiVersion: networking.k8s.io/v1 149 kind: Ingress 150 metadata: 151 annotations: 152 ingress.kubernetes.io/ssl-redirect: \u0026#39;false\u0026#39; 153 spec: 154 rules: 155 - http: 156 paths: 157 - backend: 158 service: 159 name: acme 160 path: / 161 pathType: ImplementationSpecific 162 patches: 163 - fromFieldPath: spec.id 164 toFieldPath: metadata.name 165 transforms: 166 - type: string 167 string: 168 fmt: \u0026#39;%s-ingress\u0026#39; 169 - fromFieldPath: spec.id 170 toFieldPath: spec.forProvider.manifest.metadata.name 171 - fromFieldPath: spec.parameters.namespace 172 toFieldPath: spec.forProvider.manifest.metadata.namespace 173 - fromFieldPath: spec.id 174 toFieldPath: spec.forProvider.manifest.metadata.labels.app 175 - fromFieldPath: spec.parameters.host 176 toFieldPath: spec.forProvider.manifest.spec.rules[0].host 177 - fromFieldPath: spec.id 178 toFieldPath: spec.forProvider.manifest.spec.rules[0].http.paths[0].backend.service.name 179 - fromFieldPath: spec.parameters.port 180 toFieldPath: spec.forProvider.manifest.spec.rules[0].http.paths[0].backend.service.port.number 181 - fromFieldPath: spec.parameters.kubernetesProviderConfigName 182 toFieldPath: spec.providerConfigRef.name 183 - type: ToCompositeFieldPath 184 fromFieldPath: spec.forProvider.manifest.spec.rules[0].host 185 toFieldPath: status.host That's a longer one with around 200 lines of YAML.\nLet's shift focus on the imports. One of those was common.\n1cat kcl/common.k The output is as follows.\n1schema Composition: 2 apiVersion = \u0026#34;apiextensions.crossplane.io/v1\u0026#34; 3 kind = \u0026#34;Composition\u0026#34; 4 metadata: Metadata 5 spec: Spec 6 7schema Metadata: 8 name: str 9 labels: Labels 10 11schema Spec: 12 compositeTypeRef = { 13 apiVersion = \u0026#34;devopstoolkitseries.com/v1alpha1\u0026#34; 14 kind = \u0026#34;App\u0026#34; 15 } 16 patchSets = [{ 17 name = \u0026#34;metadata\u0026#34; 18 patches = [{fromFieldPath = \u0026#34;metadata.labels\u0026#34;}] 19 }] 20 resources: [] 21 22schema Labels: 23 type: str 24 location: str 25 26schema KubernetesObject: 27 name: str 28 base = { 29 apiVersion = \u0026#34;kubernetes.crossplane.io/v1alpha1\u0026#34; 30 kind = \u0026#34;Object\u0026#34; 31 spec: KubernetesObjectSpec 32 } 33 patches: [] 34 35schema KubernetesObjectBase: 36 apiVersion = \u0026#34;kubernetes.crossplane.io/v1alpha1\u0026#34; 37 kind = \u0026#34;Object\u0026#34; 38 spec: KubernetesObjectSpec 39 40schema KubernetesObjectSpec: 41 forProvider: KubernetesObjectForProvider 42 43schema KubernetesObjectForProvider: 44 manifest: any 45 46Patches = lambda name: str -\u0026gt; [] { 47 [ 48 { 49 fromFieldPath = \u0026#34;spec.id\u0026#34; 50 toFieldPath = \u0026#34;metadata.name\u0026#34; 51 transforms = [{type = \u0026#34;string\u0026#34;, string = { fmt = \u0026#34;%s-{}\u0026#34;.format(name)}}] 52 }, 53 {fromFieldPath = \u0026#34;spec.id\u0026#34;, toFieldPath = \u0026#34;spec.forProvider.manifest.metadata.name\u0026#34;}, 54 {fromFieldPath = \u0026#34;spec.parameters.namespace\u0026#34;, toFieldPath = \u0026#34;spec.forProvider.manifest.metadata.namespace\u0026#34;}, 55 {fromFieldPath = \u0026#34;spec.id\u0026#34;, toFieldPath = \u0026#34;spec.forProvider.manifest.metadata.labels.app\u0026#34;}, 56 ] 57} 58 59ManifestSpec = \u0026#34;spec.forProvider.manifest.spec\u0026#34; Most of that file contains a schema I'm using to generate output I need. There is Composition with a few hard-coded values like apiVersion and kind, and a few sub-schemas like Metadata and Spec. Metadata, on the other hand, defines a str variable name. That's the name we saw at the very beginning. Whoever is using that schema has to define it or KCL will throw an error.\nThat continues on and on with different schemas containing either fields that are hard-coded or variables that need to be defined by whoever is using them.\nNow, the syntax itself is pretty much Json with key values being separate by = and values and types being separated by :. Objects are defined with curly braces {} and arrays with square brackets []. There are a few more things that are different from Json but, in general, if you know Json or CUE, you should be able to pick it up quickly. If you're not familiar with either of those, you'll still be able to learn it relatively fast. KCL is one of the most powerful yet one of the easiest data-structure languages I used. We'll talk about that later.\nWe can also use functions like, in this case, Patches. In KCL, functions are called lambda and this one defines a single argument name that is a str and returns an array ([, ]) of objects ({).\nFinally, that definition defines a variable ManifestSpec that I'm using as a way to avoid typing that path.\nThere's more though. Much much more than we can explore in this video, so I'll focus on only a few other aspects of KCL which we can see in the deployment.k file which is one of the imports we saw at the start.\n1cat kcl/deployment.k The output is as follows.\n1import .common 2import k8s.api.apps.v1 as k8sapps 3 4schema Deployment(common.KubernetesObject): 5 _dbEnabled: bool = False 6 _dbSecretName: str = \u0026#34;spec.id\u0026#34; 7 _providerConfigName: str = \u0026#34;spec.id\u0026#34; 8 _container = \u0026#34;{}.template.spec.containers[0]\u0026#34;.format(common.ManifestSpec) 9 name = \u0026#34;deployment\u0026#34; 10 base = common.KubernetesObjectBase{ 11 spec.forProvider.manifest = k8sapps.Deployment{ 12 spec = { 13 selector = {} 14 template = { 15 spec = { 16 containers = [{ 17 name = \u0026#34;backend\u0026#34; 18 ports = [{containerPort = 80 }] 19 livenessProbe = {httpGet = {path = \u0026#34;/\u0026#34;, port = 80 }} 20 readinessProbe = {httpGet = {path = \u0026#34;/\u0026#34;, port = 80 }} 21 resources = { 22 limits = {cpu = \u0026#34;250m\u0026#34;, memory = \u0026#34;256Mi\u0026#34; } 23 requests = {cpu = \u0026#34;125m\u0026#34;, memory = \u0026#34;128Mi\u0026#34; } 24 } 25 if _dbEnabled: 26 env = [ 27 {name = \u0026#34;DB_ENDPOINT\u0026#34;, valueFrom.secretKeyRef.key = \u0026#34;endpoint\u0026#34; }, 28 {name = \u0026#34;DB_PASSWORD\u0026#34;, valueFrom.secretKeyRef.key = \u0026#34;password\u0026#34; }, 29 {name = \u0026#34;DB_PORT\u0026#34;, valueFrom.secretKeyRef = {key = \u0026#34;port\u0026#34;, optional = True }}, 30 {name = \u0026#34;DB_USERNAME\u0026#34;, valueFrom.secretKeyRef.key = \u0026#34;username\u0026#34; }, 31 {name = \u0026#34;DB_NAME\u0026#34; }, 32 ] 33 }] 34 } 35 } 36 } 37 } 38 } 39 patches = common.Patches(\u0026#34;deployment\u0026#34;) + [ 40 { 41 fromFieldPath = \u0026#34;spec.id\u0026#34;, 42 toFieldPath = \u0026#34;{}.selector.matchLabels.app\u0026#34;.format(common.ManifestSpec) 43 }, { 44 fromFieldPath = \u0026#34;spec.id\u0026#34;, 45 toFieldPath = \u0026#34;{}.template.metadata.labels.app\u0026#34;.format(common.ManifestSpec) 46 }, { 47 fromFieldPath = \u0026#34;spec.parameters.image\u0026#34;, 48 toFieldPath = \u0026#34;{}.image\u0026#34;.format(_container) 49 }, { 50 fromFieldPath = \u0026#34;spec.parameters.port\u0026#34;, 51 toFieldPath = \u0026#34;{}.ports[0].containerPort\u0026#34;.format(_container) 52 }, { 53 fromFieldPath = \u0026#34;spec.parameters.port\u0026#34;, 54 toFieldPath = \u0026#34;{}.livenessProbe.httpGet.port\u0026#34;.format(_container) 55 }, { 56 fromFieldPath = \u0026#34;spec.parameters.port\u0026#34;, 57 toFieldPath = \u0026#34;{}.readinessProbe.httpGet.port\u0026#34;.format(_container) 58 }, 59 if _dbEnabled: 60 { 61 fromFieldPath = _dbSecretName, 62 toFieldPath = \u0026#34;{}.env[0].valueFrom.secretKeyRef.name\u0026#34;.format(_container) 63 }, { 64 fromFieldPath = _dbSecretName, 65 toFieldPath = \u0026#34;{}.env[1].valueFrom.secretKeyRef.name\u0026#34;.format(_container) 66 }, { 67 fromFieldPath = _dbSecretName, 68 toFieldPath = \u0026#34;{}.env[2].valueFrom.secretKeyRef.name\u0026#34;.format(_container) 69 }, 70 { 71 fromFieldPath = _dbSecretName, 72 toFieldPath = \u0026#34;{}.env[3].valueFrom.secretKeyRef.name\u0026#34;.format(_container) 73 }, { 74 fromFieldPath = _dbSecretName, 75 toFieldPath = \u0026#34;{}.env[4].value\u0026#34;.format(_container) 76 }, 77 { 78 fromFieldPath = _providerConfigName, 79 toFieldPath = \u0026#34;spec.providerConfigRef.name\u0026#34; 80 }, 81 ] Look at that import. KCL comes with pre-built schemas for all core Kubernetes APIs, and many others. In this case, we're importing k8s.api.apps.v1 and using it to define k8sapps.Deployment which, surprise surprise, is a Kubernetes Deployment.\nDon't be confused that the Deployment is defined inside spec.forProvider.manifest. That's a part of a Crossplane Composition which I'm not exploring in this video but only using as an example of what KCL can do.\nThe Deployment schema I'm defining here is inheriting from the KubernetesObject schema I defined in the common. That means that it inherits everything from that one and I can define only the things that are different.\nInside that schema, I defined a few variables with names prefixed with _. Those are not exported and, as such, are mutable. That's a great feature of KCL. Exported variables are immutable, non-exported are not. That might be one of the things I like the most about KCL. It's immutable when exported data is concerned, but it still allows us to mutate non-exported data. If you're not familiar with terms \u0026quot;exported\u0026quot; and \u0026quot;non-exported\u0026quot;, think of them as \u0026quot;public\u0026quot; and \u0026quot;private\u0026quot; in other languages. Only exported data is output to YAML or JSON.\nWe can see, for example, that _dbEnabled is a bool variable with the default value of False. The rest follows the similar pattern.\nThe last non-exported variable shows the usage of the format function that will replace open-closed curly braces ({}) with the value of common.ManifestSpec.\nWhat else...\nThe patches value is an interesting one. it showcases unioning of collections. Over there I'm saying that the value should be a combination of a list defined in common Patches(\u0026quot;deployment\u0026quot;) and whatever is defined below. There are some common entries that should be defined in all patches and by including items from that collection with whatever is defined below, I'm avoiding repetition.\nPatches itself, if you remember, is a function that returns an array of objects. That's the one we saw when we explore common.k file.\nThe last thing I want to show is the ability to define expressions. In this case, it is a simple if conditional that will include a few more entries into the list if _dbEnabled is True.\nWe saw only a fraction of what KCL can so, but I think that was enogh syntax for today. KCL is massive and you should spend a bit of time going though the docs yourself.\nOutside the syntax itself, there are a couple of other things I feel are worth mentioning.\nThe CLI itself is not overwhelming, yet it does have a few important features outside of the obvious one that allows us to generate YAML or JSON from KCL definitions.\n1kcl --help The output is as follows.\n1The KCL Command Line Interface (CLI). 2 3KCL is an open-source, constraint-based record and functional language that 4enhances the writing of complex configurations, including those for cloud-native 5scenarios. The KCL website: https://kcl-lang.io 6 7Usage: 8 kcl [command] 9 10Available Commands: 11 clean KCL clean tool 12 completion Generate the autocompletion script for the specified shell 13 doc KCL document tool 14 fmt KCL format tool 15 help Help about any command 16 import KCL import tool 17 lint Lint KCL codes. 18 mod KCL module management 19 play Open the kcl playground in the browser. 20 registry KCL registry management 21 run Run KCL codes. 22 server Run a KCL server 23 test KCL test tool 24 version Show version of the KCL CLI 25 vet KCL validation tool 26 27Flags: 28 -h, --help help for kcl 29 -v, --version version for kcl 30 31Use \u0026#34;kcl [command] --help\u0026#34; for more information about a command. We can, for example, use import to convert existing JSON, YAML, Go structs, Terraform, OpenAPI, Kubernetes CRDs, and a few other formats into KCL. That's great as a starting point.\nWe can use mod to initiate a KCL project, add dependencies like those we saw earlier when I showed Kubernetes schemas, to package KCL, or to pull it or push it into a registry.\nThere is the option to play with KCL by spining up a local server that will allow us to interact with KCL in a browser.\nAnd so on and so forth.\nIDE support is supperb. I'm using VS Code and the KCL extension is great. It provides syntax highlighting, auto-complete, go-to-definition, and a few other features that make working with KCL a breeze.\nThere is also integration and support for a bunch of other tools like Argo CD and Flux, CI pipelines, Hashi Vault, Terraform, and a few others.\nOkay. That's it as far as walkthrough it concerned. Let's talk about pros and cons.\nKCL Pros and Cons As a reminder, my requirements are to have a language or a DSL that is focused on data-structures, the ability to work with schemas, immutability, and for it to be easy to learn.\nKCL meets all those and so much more.\nAs a matter of fact, I could not find a single thing I don't like except that I was initially confused with the documentation but that was my fault. I was too hasty.\nIf I would have to nitpick, I'd say that the only potential, but minor, issue is that the docs contain quite a few spelling errors in the English version. I can only guess that Chinese version is better but, for obvious reasons, I cannot confirm that. Nevertheless, that's a minor issue that does not affect the quality of the docs.\nThat's it. That's the only negative thing I could find. KCL is awesome, and here are only a few out of many reasons why I think so.\nDocumentation is amazing, even though I had initial trouble understanding how it is organized. It is clear that a lot of attention is put into the design of the language and documenting every detail that mateers. I disliked it at first because of the docs but now that I went through most of it, I can safely say that was my fault. The docs are great.\nHaving all exported data immutable is just what I think we need and that's what KCL provides. The addition of mutable non-exported data is a great feature.\nIt's (relatively) easy to learn. You'll be up-and-running in no time, a day at most. You will not know everything KCL offers, that takes time, but you'll be able to do most things in a day.\nWhat else... It's powerful. I did not find anything missing. Everything I needed so far is there, and I know that there's so much more so it my needs change in the future, I'm confident that KCL will be able to accommodate them.\nNext, the VS Code plugin is great. Syntax highlighting, auto-complete, goto definitions, and all other features we might expect are there. It's a pleasure to work with KCL in VS Code.\nFinally, KCL is a project donated to Cloud Native Computing Foundation (CNCF). That makes it less prone to future license changes and makes it more likely to have a vibrant and diverse community.\nPersonally, I cannot sit on more than two chairs at the same time. Until now, I was torn between CUE and Pkl. I'll kick one of those out to gain space for KCL. Pkl, you're out. KCL is in. I might be biased though. I love CUE and KCL looks very similar to it except that it's easier to learn. It's those two now.\nThank you for watching. See you in the next one. Cheers.\nDestroy 1git checkout main ","link":"http://localhost:1313/post/kcl/","section":"post","tags":null,"title":"Exploring KCL: Configuration and Data Structure Language; CUE and Pkl Replacement?"},{"body":"You might be new to Kubernetes or you might have been working with it for a while. No matter your experience level, you might not be aware of all the Kubernetes Workload APIs.\nThere are Pods as lower level, ReplicaSets and Jobs in between, and Deployments, StatefulSets, DaemonSets, and CronJobs on top. To make things more complicated, those are workload APIs baked into Kubernetes cluster and, on top of those, we can have more, much more.\nToday we'll explore all of those. We'll see what is the purpose of each, when each of them should or should not be used for, and quite a few things.\nBuckle up! We're about to dive into all Kubernetes workload resource types.\nSetup 1git clone https://github.com/vfarcic/kubernetes-demo 2 3cd kubernetes-demo 4 5git pull 6 7git checkout workloads Make sure that Docker is up-and-running. We'll use it to create a KinD cluster.\nWatch https://youtu.be/WiFLtcBvGMU if you are not familiar with Devbox. Alternatively, you can skip Devbox and install all the tools listed in devbox.json yourself.\n1devbox shell 2 3kind create cluster --config kind.yaml 4 5kubectl create namespace a-team Containers in Kubernetes Containers are the base of everything running in Kubernetes. Everything ends up either running in a container or be managed by something running in a container.\nActually... that's not really true. It does not have to be containers. It could be WASM or it could be VMs managed by KubeVirt, or many other ways to run processes. Nevertheless, if we're talking about \u0026quot;vanilla\u0026quot; Kubernetes, it's all about containers. We package processes into container images and we run them as containers.\nHowever... you need to pay attention to this one. We cannot run containers directly. There is no option in Kubernetes to say \u0026quot;run this container\u0026quot;. Containers are always wrapped into Pods, hence that is our starting point.\nLet's take a look at what they are, how they work, and why you are unlikely to ever run them directly.\nPods in Kubernetes Pods are the base Kubernetes resources. Any type of a workload, at least among those baked in Kubernetes, ends up being a Pod.\nSo, what is a Pod?\nA Pod is a collection of one or more containers. Most of a time, a Pod contains a single container with your or someone else's application. On top of that, there might be additional containers for various purposes. For example, a service mesh might attach a container that deals with networking. Those are called side-car containers.\nHere's an example.\n1cat pod/base.yaml The output is as follows.\n1apiVersion: v1 2kind: Pod 3metadata: 4 name: silly-demo 5 labels: 6 app.kubernetes.io/name: silly-demo 7spec: 8 containers: 9 - image: ghcr.io/vfarcic/silly-demo:1.4.115 10 name: silly-demo 11 ports: 12 - containerPort: 8080 13 readinessProbe: 14 httpGet: 15 path: / 16 port: 8080 17 resources: 18 limits: 19 cpu: 250m 20 memory: 256Mi 21 requests: 22 cpu: 125m 23 memory: 128Mi That's a Pod manifest containing a list of containers. In this case there is only one based on the silly-demo image. It has a name, a list of ports, a readinessProbe that is used by Kubernetes to deduce whether it is healty, and resource limits and requests that are used to deduce how much memory and cpu we expect it to use.\nIf we apply that manifest,...\n1kubectl --namespace a-team apply --filename pod/base.yaml ...and retrieve all the Pods in that Namespace,...\n1kubectl --namespace a-team get pods The output is as follows.\n1NAME READY STATUS RESTARTS AGE 2silly-demo 1/1 Running 0 7s ...we can see that it is running.\nWe sent a request to Kubernetes API to apply a manifest containing a Pod with a single container. As a result, Kubernetes started running that container wrapped inside a Pod . That's the container with our application.\nHere's a thing though.\nYou will probably never run Pods directly. That would be foolish since Pods alone are not fault tollerant, are complicated to replace with new releases safely, and so on and so forth. Pods are building blocks that should be managed by other types of workloads.\nHere's an example. Let's say that something happened to a Pod. Let's say that it crashed, that it dissapeared. What would happen in that case?\nWe can simulate that by deleting the Pod.\n1kubectl --namespace a-team delete pod silly-demo You can probably guess what happens when we delete something.\n1kubectl --namespace a-team get pods The output is as follows.\n1No resources found in a-team namespace. It dissapeared. It's gone, and it's not coming back.\nNow, if that was our intention, that should be the expected outcome. But what if that's not what we wanted. What if our goal is to have that Pod running forever and ever, or until we choose to replace with with a newer release? What if we would like to have some kind of a \u0026quot;contract\u0026quot; that says \u0026quot;run this Pod no matter what happens.\u0026quot;\nIn that case, we would need something else. We would need a ReplicaSet.\nReplicaSets in Kubernetes A ReplicaSet is a Kubernetes resource that ensures there is always a stable set of running pods for a specific workload. It is a \u0026quot;special\u0026quot; type of resource that ensures that the contract between you and Kubernetes is maintained. It is a resource that allows us to say: \u0026quot;I want this number of Pods with those specifications. Make sure that is fullfilled no matter what happens. The world might end, but those Pods should still be running.\u0026quot;\nHere's an example.\n1cat replicaset/base.yaml The output is as follows.\n1apiVersion: apps/v1 2kind: ReplicaSet 3metadata: 4 name: silly-demo 5 labels: 6 app.kubernetes.io/name: silly-demo 7spec: 8 replicas: 3 9 selector: 10 matchLabels: 11 app.kubernetes.io/name: silly-demo 12 template: 13 metadata: 14 labels: 15 app.kubernetes.io/name: silly-demo 16 spec: 17 containers: 18 - image: ghcr.io/vfarcic/silly-demo:1.4.115 19 name: silly-demo 20 ports: 21 - containerPort: 8080 22 readinessProbe: 23 httpGet: 24 path: / 25 port: 8080 26 resources: 27 limits: 28 cpu: 250m 29 memory: 256Mi 30 requests: 31 cpu: 125m 32 memory: 128Mi This is a ReplicaSet very similar to the definition of the Pod we explored earlier. The containers section is exactly the same. The difference, however, is that the spec is now part of a template. We are telling ReplicaSet how to create Pods rather than what the Pods are. ReplicaSet is a controller that ensures that Pods we want are the Pods that will run, forever and ever.\nA second difference is that, this time, we have an option to specify the number of replicas. So, we are not only saying \u0026quot;This is the template to create Pods\u0026quot; but also \u0026quot;This is the number of Pods I want. No more, no less.\u0026quot;\nLet's apply that ReplicaSet.\n1kubectl --namespace a-team apply --filename replicaset/base.yaml The easiest way to see what's happened is through the kubectl tree plugin, so let's use it.\n1kubectl tree --namespace a-team replicaset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team ReplicaSet/silly-demo - 23s 3a-team Pod/silly-demo-7z7zt True 23s 4a-team Pod/silly-demo-hj4k2 True 23s 5a-team Pod/silly-demo-npgb6 True 23s kubectl tree plugin allows us to explore ownership relationships between Kubernetes resource. In this case, we can see that the ReplicaSet created three Pods and it now owns them. It's responsible for them. It's making sure that the \u0026quot;contract\u0026quot; is maintained meaning that it is ensuring that three Pods based on the template we specified are always up-and-running.\nAll in all, we sent a request to the Kube API to apply a ReplicaSet . Since that ReplicaSet did not exist, Kubernetes created it . The ReplicaSet controller detected a discrepancy between the desired and the actual state. We said that we want to have three Pods , and there were none owner by that ReplicaSet. Hence, the controller created three Pods based on the template we defined.\nWe can confirm that is indeed the case by deleting one of those Pods,\nReplace [...] with the name of one of the Pods (e.g., silly-demo-7z7zt)\n1kubectl --namespace a-team delete pod [...] ...and retrieving the ReplicaSet and the Pods it owns with kubectl tree.\n1kubectl tree --namespace a-team replicaset silly-demo The can see that there are three Pods, one of them being created a moment ago.\n1NAMESPACE NAME READY REASON AGE 2a-team ReplicaSet/silly-demo - 103s 3a-team Pod/silly-demo-hj4k2 True 103s 4a-team Pod/silly-demo-jw6p5 True 13s 5a-team Pod/silly-demo-npgb6 True 103s That's the fulfilment of the \u0026quot;contract\u0026quot; in action. ReplicaSet manages Pods by continuously watching their state and reacting if it differ from what was specified. As a result, it created a new Pod to replace the one we deleted.\nSo, when we removed the Pods , the ReplicaSet detected the discrepancy between what we want and what something is, and created three new Pods .\nSimilarly, we can use it to scale up and down.\nLet's take a look at a diff between the manifest we applied and a slightly modified version.\n1diff replicaset/base.yaml replicaset/replicas.yaml The output is as follows.\n18c8 2\u0026lt; replicas: 3 3--- 4\u0026gt; replicas: 5 We can see that the only difference is that the new manifest has 5 instead of 3 replicas.\nYou can probably guess what will happen if we apply it, but let's do it anyways.\n1kubectl --namespace a-team apply \\ 2 --filename replicaset/replicas.yaml 3 4kubectl tree --namespace a-team replicaset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team ReplicaSet/silly-demo - 2m32s 3a-team Pod/silly-demo-gx742 True 5s 4a-team Pod/silly-demo-hj4k2 True 2m32s 5a-team Pod/silly-demo-jw6p5 True 62s 6a-team Pod/silly-demo-npgb6 True 2m32s 7a-team Pod/silly-demo-wj52t True 5s This time, we can see that there are five Pods managed by the ReplicaSet.\nWe changed the number of replicas in the manifest and sent it to Kube API which resulted in the updated ReplicaSet running in the cluster . That ReplicaSet noticed that the desired state of five replicas is not the same as the actual state of three, and created two new Pods .\nNow, there is one important note here.\nReplicaSet ensures that the desired state specified in the spec is always the same as the actual state, except for the template. Template is used to create new Pods, and nothing else. ReplicaSet does not ensure that the state of the Pods created with the template is always correct.\nWe can demonstrate that with a modified version of the manifest, so let's take a look at the diff.\n1diff replicaset/replicas.yaml replicaset/image.yaml The output is as follows.\n118c18 2\u0026lt; - image: ghcr.io/vfarcic/silly-demo:1.4.115 3--- 4\u0026gt; - image: ghcr.io/vfarcic/silly-demo:1.4.116 We can see that the tag of the image changed from 115 to 116.\nLet's see what happens if we apply that manifest,...\n1kubectl --namespace a-team apply --filename replicaset/image.yaml ...and retrieve the dependency tree of the ReplicaSet.\n1kubectl tree --namespace a-team replicaset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team ReplicaSet/silly-demo - 3m58s 3a-team Pod/silly-demo-gx742 True 91s 4a-team Pod/silly-demo-hj4k2 True 3m58s 5a-team Pod/silly-demo-jw6p5 True 2m28s 6a-team Pod/silly-demo-npgb6 True 3m58s 7a-team Pod/silly-demo-wj52t True 91s Judging by the AGE, we can see that no new Pods were created. The ReplicaSet did nothing because we did not modify the specification itself but the template used to create Pods.\nWe can confirm that even further by retrieving the YAML specification of the Pods running in the cluster.\n1kubectl --namespace a-team get pods --output yaml | yq . The output is as follows (truncated for brevity).\n1... 2 - apiVersion: v1 3 kind: Pod 4 ... 5 spec: 6 containers: 7 - image: ghcr.io/vfarcic/silly-demo:1.4.115 8 ... We can see that the image is still 115. The changes we made to the template were not applied since, as I already mentioned, template is not used to manage the state of the Pods but only to create new Pods.\nNow, try to guess what will happen if we delete all the Pods managed by that ReplicaSet,...\n1kubectl --namespace a-team delete pods \\ 2 --selector app.kubernetes.io/name=silly-demo ...and retrieve all the dependencies of the ReplicaSet.\n1kubectl tree --namespace a-team replicaset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team ReplicaSet/silly-demo - 7m41s 3a-team Pod/silly-demo-7fhpn True 39s 4a-team Pod/silly-demo-hcrkt True 39s 5a-team Pod/silly-demo-q2ngj True 39s 6a-team Pod/silly-demo-stlvn True 39s 7a-team Pod/silly-demo-zxwxj True 39s We can see that new Pods were created. That was to be expected since, as you know, we made a contract that the number of replicas should always be five. We already saw that and the reason for deleting the Pods is not to demonstrate that ReplicaSet always maintains the spec, but what happens with changes to the template.\nLet's retrieve YAML spec of the Pods in the cluster one more time.\n1kubectl --namespace a-team get pods --output yaml | yq . 1... 2 - apiVersion: v1 3 kind: Pod 4 ... 5 spec: 6 containers: 7 - image: ghcr.io/vfarcic/silly-demo:1.4.116 8 ... We can see that, this time, the version is indeed set to the new one, to 116.\nThe ReplicaSet detected that the number of running Pods differs from the number of specified Pods, and created those that were missing. It used the template to do that.\nThere's a thing though. Just as you will probably never run Pods directly, you will probably not be creating ReplicaSets either. We already saw that ReplicaSets are limited when we changed the version of the image in the template. We had to delete the Pods for that change to be applied. That's, obviously, not ideal. We should have used Deployments instead, and we will, but only after we delete the ReplicaSet we created.\n1kubectl --namespace a-team delete \\ 2 --filename replicaset/image.yaml Here's another lesson. When we delete a Kubernetes resource, the controller behind it makes sure that all the resources that might have been managed by it are removed as well. Since the ReplicaSet was managing Pods, they should be deleted as well.\nLet's double check that.\n1kubectl --namespace a-team get all The output is as follows.\n1No resources found in a-team namespace. It's all gone. There is nothing. There is no ReplicaSet and there are no Pods.\nWe deleted the ReplicaSet which, before dissapearing completely, made sure that the dependant resources were deleted as well. In this case, those dependent resources were the Pods it created and managed . Only after the ReplicaSet controller deleted all the Pods it owned, it removed itself from the system .\nWith everything gone, we can take a look at Deployments.\nDeployments in Kubernetes Deployments are almost the same as ReplicaSets, at least when defining them is concerned.\nLet's take a diff between the two I prepared.\n1diff replicaset/image.yaml deployment/base.yaml The output is as follows.\n12c2 2\u0026lt; kind: ReplicaSet 3--- 4\u0026gt; kind: Deployment 58a9,13 6\u0026gt; minReadySeconds: 10 7\u0026gt; strategy: 8\u0026gt; rollingUpdate: 9\u0026gt; maxUnavailable: 1 10\u0026gt; maxSurge: 1 We can see that the kind changed. That could have been the only change. I also added minReadySeconds and strategy entries, not necessarily because we needed those but, rather, because those will slow down some processes we'll explore later so that we can observe them easily.\nAnyway... Let's apply that Deployment,...\n1kubectl --namespace a-team apply --filename deployment/base.yaml ...and take a look at the tree.\n1kubectl tree --namespace a-team deployment silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team Deployment/silly-demo - 10s 3a-team ReplicaSet/silly-demo-5f76b8d84 - 10s 4a-team Pod/silly-demo-5f76b8d84-c5lvb True 10s 5a-team Pod/silly-demo-5f76b8d84-c9vkt True 10s 6a-team Pod/silly-demo-5f76b8d84-j9wgh True 10s 7a-team Pod/silly-demo-5f76b8d84-rspnv True 10s 8a-team Pod/silly-demo-5f76b8d84-vchgz True 10s On the first look, creating that Deployment seems like a waste. The end result are the same five Pods we would get if we created the ReplicaSet directly.\nWe sent a request to the Kube API to apply a manifest of a Deployment , a Deployment was created , and its controller created a ReplicaSet . The controller of the ReplicaSet saw that Pods are missing, so it created them .\nNow, let's take a look at one of the things that make Deployments special. As you saw before, updating a template of a ReplicaSet does not affect existing Pods so changing the image tag did not produce the effect we wanted. Let's see what will happen if we do the same with the Deployment.\nHere's a diff of the changes I made to the Deployment manifest.\n1diff deployment/base.yaml deployment/image.yaml The output is as follows.\n123c23 2\u0026lt; - image: ghcr.io/vfarcic/silly-demo:1.4.116 3--- 4\u0026gt; - image: ghcr.io/vfarcic/silly-demo:1.4.117 The only modification is in the image value which now contains tag 117.\nA few things will happen if we update that Deployment, so I'll execute kubectl apply followed with kubectl tree to output right away the ownership tree. Since I want us to see how it progresses, we'll use viddy to output the tree every second. If you're not familiar with viddy, it is an alternative to watch.\nAnyways... Here it goes...\n1kubectl --namespace a-team apply \\ 2 --filename deployment/image.yaml \\ 3 \u0026amp;\u0026amp; viddy kubectl tree --namespace a-team \\ 4 deployment silly-demo The output is as follows.\n1... 2NAMESPACE NAME READY REASON AGE 3a-team Deployment/silly-demo - 73s 4a-team ReplicaSet/silly-demo-5f76b8d84 - 73s 5a-team  Pod/silly-demo-5f76b8d84-c5lvb True 73s 6a-team  Pod/silly-demo-5f76b8d84-c9vkt True 73s 7a-team  Pod/silly-demo-5f76b8d84-j9wgh True 73s 8a-team  Pod/silly-demo-5f76b8d84-rspnv True 73s 9a-team ReplicaSet/silly-demo-7879f7dfd7 - 9s 10a-team Pod/silly-demo-7879f7dfd7-98758 True 9s 11a-team Pod/silly-demo-7879f7dfd7-md865 True 9s Deployment created a second ReplicaSet which, initially, was set to have a single replica of a Pod that uses the new image tag. From there on, it started performing rolling updates. It started reducing the number of replicas in the old ReplicaSet and increasing the number of replicas in the new.\nThe output is as follows (cont.).\n1NAMESPACE NAME READY REASON AGE 2a-team Deployment/silly-demo - 2m3s 3a-team ReplicaSet/silly-demo-5f76b8d84 - 2m3s 4a-team ReplicaSet/silly-demo-7879f7dfd7 - 59s 5a-team Pod/silly-demo-7879f7dfd7-98758 True 59s 6a-team Pod/silly-demo-7879f7dfd7-9jqgv True 45s 7a-team Pod/silly-demo-7879f7dfd7-dwvz9 True 31s 8a-team Pod/silly-demo-7879f7dfd7-f8lfx True 45s 9a-team Pod/silly-demo-7879f7dfd7-md865 True 59s At the end of the process, the old ReplicaSet was scaled to zero replicas, and the new one, the one with the new image tag, was scaled to five replicas. That was an example of an effeortless rolling updates and is, arguably, the main reason why we use Deployments.\nStop watching by pressing ctrl+c.\nHere's what happened.\nWe modified the manifest of the Deployment to use image tag 117 and applied it to the Kube API . That resulted in updated Deployment which, in turn, realized that the tag changed and created a second ReplicaSet set to have a single replica . The new ReplicaSet created a single Pod . From there on, Deployment started updating both ReplicaSets. It was reducing the number of replicas in the old one and increasing the number of replicas in the new. As a result, the old ReplicaSet started removing Pods while the new one was creating them . Eventually, all Pods ended up being managed by the new ReplicaSet while the old one ended up managing none.\nThe new version of the application was released safely and without downtime. Huray!\nThere's one more thing related to Deployments we need to explore. We need to talk about volumes since they are one of the main reasons why you might choose NOT to use Deployments.\nDeployment Volumes in Kubernetes So far, we established that we should neither use Pods nor ReplicaSets. They are important building blocks and understanding them is important, but we can safely focus on Deployments which will create and manage those for us.\nBut, depending on the type of the application we have, Deployments might or might not be a good choice. The main reason why we might use them or discard them lies in how they manage volumes which are, most of the time, external storage.\nLet's take a look at yet another example.\n1cat deployment/volume.yaml The output is as follows.\n1--- 2apiVersion: v1 3kind: PersistentVolumeClaim 4metadata: 5 name: silly-claim 6spec: 7 accessModes: 8 - ReadWriteOnce 9 resources: 10 requests: 11 storage: 1Gi 12--- 13apiVersion: apps/v1 14kind: Deployment 15metadata: 16 name: silly-demo 17 labels: 18 app.kubernetes.io/name: silly-demo 19spec: 20 replicas: 5 21 minReadySeconds: 10 22 strategy: 23 rollingUpdate: 24 maxUnavailable: 1 25 maxSurge: 1 26 selector: 27 matchLabels: 28 app.kubernetes.io/name: silly-demo 29 template: 30 metadata: 31 labels: 32 app.kubernetes.io/name: silly-demo 33 spec: 34 containers: 35 - image: ghcr.io/vfarcic/silly-demo:1.4.117 36 name: silly-demo 37 ports: 38 - containerPort: 8080 39 readinessProbe: 40 httpGet: 41 path: / 42 port: 8080 43 resources: 44 limits: 45 cpu: 250m 46 memory: 256Mi 47 requests: 48 cpu: 125m 49 memory: 128Mi 50 volumeMounts: 51 - mountPath: /cache 52 name: silly-cache 53 volumes: 54 - name: silly-cache 55 persistentVolumeClaim: 56 claimName: silly-claim This manifest defines two resources.\nThe first one is a PersistentVolumeClaim. I'll explore volumes in one of the upcoming videos. For now, what matters, is that PersistentVolumeClaim is a way for us to claim a volume, to request storage that can be attacted to containers in a Pod.\nFurther down, we have a modified version of the Deployment we used earlier.\nAt the very bottom of it is the list of volumes we'd like to have. There's only one called silly-cache which references the persistentVolumeClaim named silly-claim.\nInside the containers section we are mounting the silly-cache volume into the /cache directory inside the container.\nAs a result, all 5 replicas of our application should have external storage mounted.\nHere's a question for you. How many volumes do you think will be created for those five replicas?\nDo not answer that question aloud. It's not a good idea to talk to your monitor. Keep the number to yourself while we take a look at the result.\n1kubectl --namespace a-team apply \\ 2 --filename deployment/volume.yaml The output is as follows.\n1persistentvolumeclaim/silly-claim created 2deployment.apps/silly-demo configured Let's wait for a few moments until rolling updates are finished.... and take a look at the pods and persistentvolumes that were created.\n1kubectl --namespace a-team get pods,persistentvolumes The output is as follows.\n1NAME READY STATUS RESTARTS AGE 2pod/silly-demo-77fb46fcfc-5bqw5 1/1 Running 0 27s 3pod/silly-demo-77fb46fcfc-5lgdf 1/1 Running 0 27s 4pod/silly-demo-77fb46fcfc-cdtrl 1/1 Running 0 11s 5pod/silly-demo-77fb46fcfc-hpr24 1/1 Running 0 11s 6pod/silly-demo-7879f7dfd7-98758 1/1 Running 0 1s 7 8NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE 9persistentvolume/pvc-903d6f48-bb49-4c77-995f-b4ddd773868f 1Gi RWO Delete Bound a-team/silly-claim standard \u0026lt;unset\u0026gt; 23s As expected, there are five Pods. If you see more, you were inpatient and did not wait until rolling updates finished.\nThe number of Pods does not matter right now. What does matter is that there is only one persistentvolume. All the replicas of the application, the Pods, got the same volume attached. They are all sharing the same storage.\nSo, we sent a request to Kube API to apply a PersistentVolumeClaim and a Deployment so those were created or updated inside the cluster . The Deployment itself created a ReplicaSet which created five Pods . Since the Pods themselves ware instructed, through the template in the Deployment, to use the claim to request a volume, a PersistentVolume was created and attached to all the Pods .\nNow, depending on the type of the application we're using, that might be just what we want, or unacceptable. As a rule of thumb, stateless applications should be managed through Deployments. On the other hand, stateful applications tend to need separate storage for each replicas as a way to avoid potential issues when writing files to disk. Hence, we need to look for a different way to run stateful apps.\nWe'll do that right after we destroy the deployment and the volume we just created.\n1kubectl --namespace a-team delete \\ 2 --filename deployment/volume.yaml StatefulSets in Kubernetes StatefulSets are similar to Deployments, at least in the way we define them.\nHere's an example.\n1cat statefulset/base.yaml The output is as follows.\n1--- 2apiVersion: apps/v1 3kind: StatefulSet 4metadata: 5 name: silly-demo 6 labels: 7 app.kubernetes.io/name: silly-demo 8spec: 9 replicas: 2 10 minReadySeconds: 10 11 updateStrategy: 12 rollingUpdate: 13 maxUnavailable: 1 14 selector: 15 matchLabels: 16 app.kubernetes.io/name: silly-demo 17 template: 18 metadata: 19 labels: 20 app.kubernetes.io/name: silly-demo 21 spec: 22 containers: 23 - image: ghcr.io/vfarcic/silly-demo:1.4.117 24 name: silly-demo 25 ports: 26 - containerPort: 8080 27 readinessProbe: 28 httpGet: 29 path: / 30 port: 8080 31 resources: 32 limits: 33 cpu: 250m 34 memory: 256Mi 35 requests: 36 cpu: 125m 37 memory: 128Mi 38 volumeMounts: 39 - mountPath: /cache 40 name: silly-claim 41 volumeClaimTemplates: 42 - apiVersion: v1 43 kind: PersistentVolumeClaim 44 metadata: 45 name: silly-claim 46 spec: 47 accessModes: 48 - ReadWriteOnce 49 resources: 50 requests: 51 storage: 1Gi On the first look, this is almost the same as the Deployment we used earlier, with three notable differences.\nFirst of all, there is no need to define a PersistentVolumeClaim. We'll see why is that so.\nNext is the obvious one with kind being set to StatefulSet.\nFinally, the most important difference is in the way StatefulSets deal with volumes. While Deployments specify which volume claims to use, StatefulSets define volumeClaimTemplates. That's similar to how ReplicaSets contain Pod templates that allow them to create Pods they need. Similarly, StatefulSets have volumeClaimTemplates that create volume claims for each replica. In this case, we have a single claim template that is essentially the same as the PersistentVolumeClaim we had as a separate resource in the manifest with the Deployment.\nWhat matters for now, is that StatefulSets know how to create PersistentVolumeClaims and that enables them to treat volumes in a very different way from Deployments.\nLet's see it in action by applying the StatefulSet,...\n1kubectl --namespace a-team apply --filename statefulset/base.yaml ...and watching the tree.\n1viddy kubectl tree --namespace a-team statefulset silly-demo Before we get back to volumes, there is one important thing we can observe.\nThe names of the Pods are, this time, predictable. We specified that we want two replicas of silly-demo. Unlike with Deployments the names are predictable and the order how Pods are created and destroyed are predictable as well.\nThe output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team StatefulSet/silly-demo - 9s 3a-team ControllerRevision/silly-demo-6848df9f6f - 9s 4a-team Pod/silly-demo-0 True 9s Since we requested two replicas, StatefulSet created the frist one with the name silly-demo-0,...\nThe output is as follows (cont.).\n1NAMESPACE NAME READY REASON AGE 2a-team StatefulSet/silly-demo - 64s 3a-team ControllerRevision/silly-demo-6848df9f6f - 64s 4a-team Pod/silly-demo-0 True 64s 5a-team Pod/silly-demo-1 True 44s ...and then the second one called silly-demo-1.\nStop watching by pressing ctrl+c.\nSo, the names are predictable and the order Pods are created and, as you will see soon, destroyed is predictable as well. That's important when working with stateful apps like, for example, databases.\nAnother note is that, unlike Deployments, StatefulSets do not create and manage ReplicaSets. Instead, StatefulSets manage Pods directly.\nLet's get back Pods and PersistentVolumes.\n1kubectl --namespace a-team get pods,persistentvolumes The output is as follows.\n1NAME READY STATUS RESTARTS AGE 2pod/silly-demo-0 1/1 Running 0 88s 3pod/silly-demo-1 1/1 Running 0 68s 4 5NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS VOLUMEATTRIBUTESCLASS REASON AGE 6persistentvolume/pvc-254a27c2-2c82-44bc-afb2-b95e93b3c2d5 1Gi RWO Delete Bound a-team/silly-claim-silly-demo-0 standard \u0026lt;unset\u0026gt; 85s 7persistentvolume/pvc-7b8eccc9-ad50-4387-b3d8-4f6664e86bc6 1Gi RWO Delete Bound a-team/silly-claim-silly-demo-1 standard \u0026lt;unset\u0026gt; 65s This is the important different. This time, containers in each pod got a separate persistentvolume, a separate storage attached.\nHere's what we did and what happened.\nWe sent a request to Kube API to apply a StatefulSet . Since that StatefulSet did not exist in the cluster, a new one was created . Since we specified that it should contain two replicas, two Pods were created by the StatefulSet controller . In parallel, the StatefulSet created two PersistentVolumeClaims , which claimed two volumes and atteched one to each of the Pods .\nLet's see what happens if, for example, we increate the number of replicas.\nHere's the diff between the old and the new manifest.\n1diff statefulset/base.yaml statefulset/replicas.yaml The output is as follows.\n19c9 2\u0026lt; replicas: 2 3--- 4\u0026gt; replicas: 5 The only change is that the number of replicas jumped from 2 to 5.\nLet's apply that and watch the changes of the tree.\n1kubectl --namespace a-team apply \\ 2 --filename statefulset/replicas.yaml \\ 3 \u0026amp;\u0026amp; viddy kubectl tree --namespace a-team \\ 4 statefulset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team StatefulSet/silly-demo - 3m25s 3a-team ControllerRevision/silly-demo-6848df9f6f - 3m25s 4a-team Pod/silly-demo-0 True 3m25s 5a-team Pod/silly-demo-1 True 3m5s 6a-team Pod/silly-demo-2 - 3s StatefulSet continues working in an ordered and predictable manner. It started by creating the third replica (silly-demo-2) since two were already there. Then it created the fourth (silly-demo-3),...\nThe output is as follows (cont.).\n1NAMESPACE NAME READY REASON AGE 2a-team StatefulSet/silly-demo - 4m5s 3a-team ControllerRevision/silly-demo-6848df9f6f - 4m5s 4a-team Pod/silly-demo-0 True 4m5s 5a-team Pod/silly-demo-1 True 3m45s 6a-team Pod/silly-demo-2 True 43s 7a-team Pod/silly-demo-3 True 23s 8a-team Pod/silly-demo-4 - 3s ...and the fifth (silly-demo-4).\nAs you can probably guess, even though we cannot see that on the screen, it also created three new PersistentVolumeClaims which resulted in three new PersistentVolumes, one for each new Pod.\nStop watching by pressing ctrl+c.\nFinally, just as creation of Pods is predictable by always starting from the zero indexed Pod, deletion is predicatable as well. Depending on whether we reduce the number of replicas or delete the whole StatefulSet and all the Pods it manages, deletion always starts from the one with the highest index and continues down.\nLet's delete the StatefulSet.\n1kubectl --namespace a-team delete \\ 2 --filename statefulset/replicas.yaml I'll leave it to you to observe how Pods get deleted. Once you're done, we'll explore a completely different, yet somehow the same type of Kubernetes workloads.\nDaemonSets in Kubernetes The next Kubernetes workload type is DaemonSet. I already prepared a manifest. Instead taking a look at it directly, we'll explore the difference compared with the one that contains the Deployment.\n1diff deployment/base.yaml daemonset/base.yaml The output is as follows.\n12c2 2\u0026lt; kind: Deployment 3--- 4\u0026gt; kind: DaemonSet 58,13d7 6\u0026lt; replicas: 5 7\u0026lt; minReadySeconds: 10 8\u0026lt; strategy: 9\u0026lt; rollingUpdate: 10\u0026lt; maxUnavailable: 1 11\u0026lt; maxSurge: 1 There is the obvious change in the kind that is now set to DaemonSet, but also in some fields missing. It would not make sense to specify replicas, strategy and quite a few other things that might be useful for Deployments or StatefulSets. \u0026quot;Why?\u0026quot; you might ask. Well... Let me show it in action before I answer that question.\nLet's apply the manifest,...\n1kubectl --namespace a-team apply --filename daemonset/base.yaml ...and output the ownership tree.\n1kubectl tree --namespace a-team daemonset silly-demo The output is as follows.\n1NAMESPACE NAME READY REASON AGE 2a-team DaemonSet/silly-demo - 4s 3a-team ControllerRevision/silly-demo-5f76b8d84 - 4s 4a-team Pod/silly-demo-6w5fb True 4s 5a-team Pod/silly-demo-mhpcj True 4s 6a-team Pod/silly-demo-wrdrs True 4s This might look strange. The DaemonSet created three Pods even though we did not specify any number of replicas. Why are there three Pods and not one, or five, or any other number?\nTo understand that, we need to output the nodes of the cluster.\n1kubectl get nodes The output is as follows.\n1NAME STATUS ROLES AGE VERSION 2kind-control-plane Ready control-plane 31m v1.29.2 3kind-worker Ready \u0026lt;none\u0026gt; 30m v1.29.2 4kind-worker2 Ready \u0026lt;none\u0026gt; 30m v1.29.2 5kind-worker3 Ready \u0026lt;none\u0026gt; 30m v1.29.2 If we exclude the control-plane node that, typically, should not have any workloads, there are three worker nodes. That can lead you to guess what DaemonSets do. They guarantee that a Pod will run in every single worked node of a cluster.\nHere's what happened.\nWe have a Kubernetes cluster with a control plane and three worker nodes . We applied a DaemonSet which, in turn, ensured that there is a Pod based on the template in every single node of the cluster . If we would add more nodes to the cluster , the DaemonSet would ensure that a Pod is running there as well . If we would remove a node ... nothing would happen. We would have one less node and with it gone, one less Pod.\nAll that begs yet another \u0026quot;Why?\u0026quot;. Why would any want an instance running in every single node? Well... That's very useful for types of applications that are specific to nodes. An example could be log collection. Tools like Fluentd run as DaemonSets since they need to collect and ship logs from each node of a cluster. A similar need exists for observability tools and many others. More often than not, DaemonSets are used by third-party tools, but there might be cases when your apps should also run on every node of a cluster.\nWe're not yet done though. There are a few other workload APIs we should explore, so let's remove the DaemonSet and move on.\n1kubectl --namespace a-team delete --filename daemonset/base.yaml The next in line are Jobs.\nJobs in Kubernetes All the workload types we explored so far are meant to be used for long running processes. If a main process in a container stops, the container itself would stop and, with it, Pod would be considered unhealthy. If such a Pod was created through a ReplicaSet, StatefulSet, or a DaemonSet, it would be recreated. All those assume that the contract means that they should ensure that specific number of Pods should run forever and ever. More often than not, that's what we want.\nHowever, there are cases when we want to execute a process that does something and, once it's finished, shuts itself down and dissapears. A typical use-case for such processes would be batch processing, pipeline builds, and other one-shot actions.\nFortunately, Kubernetes has Jobs which are a type workload designed to do just that.\nLet's take a look at yet another manifest.\n1cat job/base.yaml The output is as follows.\n1apiVersion: batch/v1 2kind: Job 3metadata: 4 name: silly-demo 5 labels: 6 app.kubernetes.io/name: silly-demo 7spec: 8 template: 9 metadata: 10 labels: 11 app.kubernetes.io/name: silly-demo 12 spec: 13 restartPolicy: OnFailure 14 containers: 15 - image: cgr.dev/chainguard/bash 16 name: silly-demo 17 command: [\u0026#34;echo\u0026#34;, \u0026#34;What is this?\u0026#34;] This is a Job. It is very similar to other workload types with a template that allows us to define the characteristics that will enable it to spin up Pods. The only notable difference is that we can set restartPolicy to be either Never or, as is the case of this example, to OnFailure. As a comparison, Deployments have that field set to hard-coded value Always.\nThat field alone gives a clear indication what a Job does. While Pods created through Deployments are always restarted or recreated, no matter the reason they might fail, those created through Jobs are either never restarted, or only in case they fail. If the process running in a container ends running successfully by emitting status code 0, Job that created it will make no attempt to start it again.\nLet's apply it,...\n1kubectl --namespace a-team apply --filename job/base.yaml ...and take a look at the ownership tree.\n1kubectl tree --namespace a-team job silly-demo 1NAMESPACE NAME READY REASON AGE 2a-team Job/silly-demo - 11s 3a-team Pod/silly-demo-vn5xg False PodCompleted 11s A Pod was created and it is no longer READY. That's okay since it completed doing whatever it was supposed to do, which, in this case, was to output \u0026quot;What is this?\u0026quot; message.\nNow, even though the Pod is no longer running, we can still access the logs it created.\n1kubectl --namespace a-team logs \\ 2 --selector app.kubernetes.io/name=silly-demo The output is as follows.\n1What is this? There we go. We sent a request to Kube API to apply a Job , a Job was created and its controller created a Pod . The container in that Pod started a process that, eventually, finished doing whatever it was doing and returned exit code 0 . That was it. Unlike other workload types, the Job did not try to restart it or to recreate it.\nThe job finished, so let's remove it before we move into a next workload type.\n1kubectl --namespace a-team delete --filename job/base.yaml CronJobs in Kubernetes Being able to run run Job as we just did is often great when creation of those jobs is triggered by something. For example, if we would run CI/CD pipelines in Kubernetes, those pipelines would be Jobs triggered by some events like pushing code to a Git repo, container images to a registry, uploads to an S3 drive, and so on and so forth. There are, however, cases, when we do not have such triggers but would like to run jobs periodically. A good example would be scheduled backups. In those cases, we can use CronJobs.\nHere's an example.\n1cat cronjob/base.yaml The output is as follows.\n1apiVersion: batch/v1 2kind: CronJob 3metadata: 4 name: silly-demo 5 labels: 6 app.kubernetes.io/name: silly-demo 7spec: 8 schedule: \u0026#34;*/1 * * * *\u0026#34; 9 jobTemplate: 10 spec: 11 template: 12 metadata: 13 labels: 14 app.kubernetes.io/name: silly-demo 15 spec: 16 restartPolicy: OnFailure 17 containers: 18 - image: cgr.dev/chainguard/bash 19 name: silly-demo 20 command: [\u0026#34;echo\u0026#34;, \u0026#34;What is this?\u0026#34;] The kind is, this time, CronJob. It contains schedule that uses a standard Linux cronjob syntax to define frequency of execution, and a jobTemplate. In this case, the Job will run every minute.\nJust as Deployments are types of resources that manage ReplicaSets, CronJobs manage Jobs. So, jobTemplate tells CronJob how to create Jobs and, in this case, the content of the template is pretty much the same as what we had in the Job.\nLet's apply it,...\n1kubectl --namespace a-team apply --filename cronjob/base.yaml ...and take a look at the CronJobs inside the a-team Namespace.\n1kubectl --namespace a-team get cronjobs The output is as follows.\n1NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE 2silly-demo */1 * * * * False 0 22s 35s You can probably guess what those fields mean, so let's watch the Pods that should be created by that Job.\n1viddy kubectl --namespace a-team get pods The output is as follows.\n1NAME READY STATUS RESTARTS AGE 2silly-demo-28521541-wzj7f 0/1 Completed 0 63s 3silly-demo-28521542-pt5hp 0/1 ContainerCreating 0 3s 4... We can see that the first Pod was created. Let's fast-forward to the next minute and... there we go. The second Pod was created... and the third, and so on and so forth.\nStop watching by pressing ctrl+c.\nWe explored all workload resources baked into Kubernetes, but there's more, much more. Baked-in resource types are only the tip of the iceberg.\nLet's destroy the CronJob before we move on.\n1kubectl --namespace a-team delete --filename cronjob/base.yaml Goodbye (For Now) Resource definitions baked into Kubernetes are more more like building blocks than something we should use dirctly. We can accomplish so much more if we add projects that use those blocks and provide higher levels of abstractions. Unfortunately, we don't have time to go through those, especially since the number of those extensions and abstractions is close to infinite. We could be using Knative, Cloud-Native PostgreSQL or CNPG, and many other third-party CRDs and controllers or we could build our own. You can find examples of quite a few of such solutions on this channel.\nNow I have a question for you. Was this video useful? Would it make sense to continue through other Kubernetes API groups? If it does, which one would you like to see next? Service API? Config and storage APIs? Something else?\nLet me know in the comments.\nThank you for watching. See you in the next one. Cheers.\nDestroy 1kind delete cluster 2 3git checkout main 4 5exit ","link":"http://localhost:1313/post/workloads/","section":"post","tags":null,"title":"Mastering Kubernetes: Dive into Workloads APIs"},{"body":"Testing is important, no matter what you're working on. If you write Java code, you need to test it. If you're managing infrastructure, you need to test it. If you're working with IoT, you need to test it. There is no excuse not to test while working and before moving it to production. Testing while working allows us to work more efficiently. Testing before moving something to production allows us to have confidence that it will not explode.\nNow, today's session will not focus on all the ways we can test something. I will not go into mad rant explaining the importance of test-driven development, test automation, CI/CD pipelines, or anything else related to testing.\nInstead, today I want to focus on testing Kubernetes resources.\nMost people just write Kubernetes manifests directly as YAML or package them into Helm charts, or Kustomize, or whatever else we might be using. Once it's done, those people would just deploy that something to a Kubernetes cluster. That strategy tends to be successful if you are religious since it relies on prayer to the deity of choice to be successful. \u0026quot;Dear God, bless those resources and let them be run successfully. I have no idea what will happen so I will rely on your infinite wisdom.\u0026quot;\nGood engineers test everything, all the time. They test continuously while developing, and they test through CI/CD pipelines after pushing to Git. The former allows us to write code faster, and the latter gives us needed confidence. Now, when I said that good engineers test everything, I meant everything, and that includes Kubernetes resources.\nHowever, we have a problem.\nWhen testing is concerned, the situation with Kubernetes is pathetic. We do not have many tools that can help us test Kubernetes resources. We certainly have plethora of tools to test applications running in Kubernetes, but we do not have much to test Kubernetes resources themselves.\nThere's hope or, to be more precise, there was hope that died. A while ago, we got KUTTL.\nI already explored KUTTL in that video, so I won't go into details here. Watch it if you haven't already. The link is in the description.\nThe problem with KUTTL is... No! Wait... It would be easier to explain what is NOT a problem with KUTTL. The main positive thing about KUTTL is that it exists. It's one of the few, if not the only tool of that kind thus making it great simply because there is nothing better.\nKUTTL is a project that is not maintained. The last release was published over a year ago. That, by itself, is a huge red flag. On top of that, it is missing too many features that I feel are critical.\nNo patching No templating Insufficient output No catch. I, for example, am in a desperate need of a mechanism to patch or update existing resources, I need some form of templating to avoid repetitions, I need better outputs, and I need some form of a catch mechanism that will allow me to execute additional steps when a test fails so that I get more information about the failure.\nThat's my list of missing features and I think I found a replacement that meets all of those. That replacement is Kyverno Chainsaw.\nJust as my requirements are the result of frustration when working with KUTTL. Luckily for me, Kyverno folks faced similar obstacles. They needed a tool to test Kyverno, they used KUTTL, and they quickly hit walls that prevented them to do what they wanted to do.\nBut... There's a big difference between me and Kyverno folks. While my dissapointed with KUTTL resulted in me crying at night, they did something about it. They tried contributing to KUTTL, but that did not work out for a variety of reasons. Then they decided to create a new tool, a new project, and that is today known as Kyverno Chainsaw.\nIt's great, so let's see it in action.\nSetup Make sure that Docker is up and running.\n1git clone https://github.com/vfarcic/crossplane-sql 2 3cd crossplane-sql 4 5git pull 6 7git checkout chainsaw Watch Nix for Everyone: Unleash Devbox for Simplified Development if you are not familiar with Devbox. Alternatively, you can skip Devbox and install all the tools listed in devbox.json yourself.\n1devbox shell 2 3task cluster-create Kyverno Chainsaw in Action Here's what I do when I want to start working on this repo.\n1chainsaw test The output is as follows (truncated for brevity).\n1... 2=== NAME chainsaw/azure#01 3 | 12:09:52 | azure | @cleanup | DELETE | DONE | v1/Namespace @ chainsaw-alive-weevil 4=== NAME chainsaw/azure 5 | 12:09:53 | azure | @cleanup | DELETE | DONE | v1/Namespace @ chainsaw-touching-treefrog 6=== NAME chainsaw/aws 7 | 12:09:56 | aws | @cleanup | DELETE | DONE | v1/Namespace @ chainsaw-merry-starfish 8--- PASS: chainsaw (0.00s) 9 --- PASS: chainsaw/azure#01 (15.37s) 10 --- PASS: chainsaw/azure (15.95s) 11 --- PASS: chainsaw/aws (18.90s) 12PASS 13Tests Summary... 14- Passed tests 3 15- Failed tests 0 16- Skipped tests 0 17Done. That run all the tests related to that project which, by the way, are a few Crossplane Compositions.\nThat's irrelevant for this story though. I'd run similar set of tests for any type of Kubernetes resources.\nActually, I do not run tests like that but, instead, I run them through Task which executes tests every time I make a change to my Kubernetes manifests or tests themselves. That is also beyond the point. What matters is that I can execute tests whenever I want to validate any of my Kubernetes resources and I can execute something similar from pipeline builds triggered when I push a change to a Git repo.\nTODO: Thumbnail: Z7EnwBaJzCk\nIf you are not familiar with Task, you should be. You can watch Say Goodbye to Makefile - Use Taskfile to Manage Tasks in CI/CD Pipelines and Locally to get familiar with it. It's awesome.\nNow, if you are familiar with KUTTL, you might be thinking that Chainsaw is the same, just with a nicer output. If that's what you're thinking, you're wrong. Chainsaw is so much more, and we can see some of that by exploring one of my Test definitions.\n1cat tests/aws/chainsaw-test.yaml The output is as follows (truncated for brevity).\n1apiVersion: chainsaw.kyverno.io/v1alpha1 2kind: Test 3metadata: 4 name: aws 5spec: 6 template: true 7 bindings: 8 - name: hyperscaler 9 value: aws 10 - name: cluster 11 value: eks 12 steps: 13 - try: 14 - apply: 15 file: ../common/install.yaml 16 ... Just as KUTTL, Chainsaw allows us to run tests based on naming convention where applying resources and testing them is executed based on file name ordering and with asserts always being named assert.\nHowever, with the Test manifest like this one, we can gain more control over what is executed, when it's executed, what something does, and so on and so forth.\nAt the very top I'm specifying that Chainsaw should expect some files to serve as template. Further on, there are two bindings. In this specific case, hyperscaler is set to aws and clusterto eks. Templates and bindings allow us to avoid repetition. In my case, there are manifests for Azure, Google Cloud, and, as we can see in this example, for AWS. Since those are often very similar with only changes to a few values, I could define all three once and let Chainsaw replace those bindings with actual values. That's, in a way, similar to what we would normally do with Helm or any other templating engine.\nFurther on, we have the try section that starts with the instruction to apply whatever is defined in ../common/install.yaml, so let's take a look at that file.\n1cat tests/common/install.yaml The output is as follows.\n1--- 2apiVersion: v1 3kind: Secret 4metadata: 5 name: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler, \u0026#39;password\u0026#39;])) 6data: 7 password: cG9zdGdyZXM= 8--- 9apiVersion: devopstoolkitseries.com/v1alpha1 10kind: SQLClaim 11metadata: 12 name: my-db 13spec: 14 id: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler])) 15 compositionSelector: 16 matchLabels: 17 provider: ($hyperscaler) 18 db: postgresql 19 parameters: 20 version: \u0026#34;13.4\u0026#34; 21 size: medium This is where I'm using bindings we saw in the Test file.\nSince all three variations are similar, instead of defining three separate files to apply them, I have this one manifest that uses Chainsaw bindings and functions.\nFor example, the name of the secret should be my-db-aws-password for AWS, my-db-azure-password for Azure, and my-db-google-password for GCP. So, instead of hardcoding those values, I'm using the join function that combines hard-coded my-db and password with the hyperscaler value defined in Test bindings.\nWe can observe a similar situation with SQLClaim spec.id field that contains a similar join function as the value. Then there is spec.compositionSelector.matchLabels.provider that only has the hyperscaler value.\nAs I said, that's similar to what we would expect with something like Helm templating. Now, just to avoid confusion that might lead you to say \u0026quot;but why don't we use Helm templating instead of Chainsaw's syntax\u0026quot;... You'll see later that there is much more to it than simple templating.\nNow, to be clear, that join function, and many other features of Chainsaw can look scary. The reaction might be \u0026quot;This is silly\u0026quot; or \u0026quot;This is too much to learn\u0026quot; or \u0026quot;This does not make sense, I should write tests in a different way\u0026quot;. Those are some of the thoughts I had when I saw Chainsaw for the first time, and we'll comment on how I feel about them now near the end of the video.\nLet's go back to the Test file and see what else I have there.\n1cat tests/aws/chainsaw-test.yaml The output is as follows (truncated for brevity).\n1apiVersion: chainsaw.kyverno.io/v1alpha1 2kind: Test 3metadata: 4 name: aws 5spec: 6 ... 7 steps: 8 - try: 9 - apply: 10 file: ../common/install.yaml 11 - assert: 12 file: ../common/assert-install.yaml 13 - assert: 14 file: assert-install.yaml 15 ... After install.yaml is applied, I want to perform some tests. Some of those are fairly similar no matter whether they are run for AWS resources of something else, while others are very specific to AWS. To solve that, I have two assert entries. The common asserts are in the assert-install.yaml file, so let's take a look at it.\n1cat tests/common/assert-install.yaml The output is as follows.\n1--- 2apiVersion: devopstoolkitseries.com/v1alpha1 3kind: SQLClaim 4metadata: 5 name: my-db 6spec: 7 compositionRef: 8 name: (join(\u0026#39;-\u0026#39;, [$hyperscaler, \u0026#39;postgresql\u0026#39;])) 9 compositionSelector: 10 matchLabels: 11 db: postgresql 12 provider: ($hyperscaler) 13 id: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler])) 14 parameters: 15 size: medium 16 version: \u0026#34;13.4\u0026#34; 17 resourceRef: 18 apiVersion: devopstoolkitseries.com/v1alpha1 19 kind: SQL This one uses the same join function as before but, this time, it is used to generate the asssert manifest. The final output is what is compared against the actual resources in the cluster meaning that it will try to find SQLClaim that matches that exact specification.\nThe second assert manifest is assert-install.yaml in the aws directory, so let's take a look at that one as well.\n1cat tests/aws/assert-install.yaml The output is as follows.\n1--- 2apiVersion: devopstoolkitseries.com/v1alpha1 3kind: SQL 4metadata: 5 labels: 6 crossplane.io/claim-name: my-db 7spec: 8 claimRef: 9 apiVersion: devopstoolkitseries.com/v1alpha1 10 kind: SQLClaim 11 name: my-db 12 compositionRef: 13 name: aws-postgresql 14 compositionSelector: 15 matchLabels: 16 db: postgresql 17 provider: aws 18 compositionUpdatePolicy: Automatic 19 id: my-db-aws 20 parameters: 21 size: medium 22 version: \u0026#34;13.4\u0026#34; 23 resourceRefs: 24 - apiVersion: ec2.aws.upbound.io/v1beta1 25 kind: InternetGateway 26 name: my-db-aws 27 - apiVersion: ec2.aws.upbound.io/v1beta1 28 kind: MainRouteTableAssociation 29 name: my-db-aws 30 - apiVersion: ec2.aws.upbound.io/v1beta1 31 kind: RouteTableAssociation 32 name: my-db-aws-1a 33 - apiVersion: ec2.aws.upbound.io/v1beta1 34 kind: RouteTableAssociation 35 name: my-db-aws-1b 36 - apiVersion: ec2.aws.upbound.io/v1beta1 37 kind: RouteTableAssociation 38 name: my-db-aws-1c 39 - apiVersion: ec2.aws.upbound.io/v1beta1 40 kind: RouteTable 41 name: my-db-aws 42 - apiVersion: ec2.aws.upbound.io/v1beta1 43 kind: Route 44 name: my-db-aws 45 - apiVersion: ec2.aws.upbound.io/v1beta1 46 kind: SecurityGroupRule 47 name: my-db-aws 48 - apiVersion: ec2.aws.upbound.io/v1beta1 49 kind: SecurityGroup 50 name: my-db-aws 51 - apiVersion: ec2.aws.upbound.io/v1beta1 52 kind: Subnet 53 name: my-db-aws-a 54 - apiVersion: ec2.aws.upbound.io/v1beta1 55 kind: Subnet 56 name: my-db-aws-b 57 - apiVersion: ec2.aws.upbound.io/v1beta1 58 kind: Subnet 59 name: my-db-aws-c 60 - apiVersion: ec2.aws.upbound.io/v1beta1 61 kind: VPC 62 name: my-db-aws 63 - apiVersion: kubernetes.crossplane.io/v1alpha1 64 kind: ProviderConfig 65 name: my-db-aws-sql 66 - apiVersion: kubernetes.crossplane.io/v1alpha2 67 kind: Object 68 name: my-db-aws-secret 69 - apiVersion: postgresql.sql.crossplane.io/v1alpha1 70 kind: ProviderConfig 71 name: my-db-aws 72 - apiVersion: rds.aws.upbound.io/v1beta1 73 kind: Instance 74 name: my-db-aws 75 - apiVersion: rds.aws.upbound.io/v1beta1 76 kind: SubnetGroup 77 name: my-db-aws 78--- 79apiVersion: ec2.aws.upbound.io/v1beta1 80kind: InternetGateway 81metadata: 82 annotations: 83 crossplane.io/composition-resource-name: gateway 84 labels: 85 crossplane.io/claim-name: my-db 86 name: my-db-aws 87 ownerReferences: 88 - apiVersion: devopstoolkitseries.com/v1alpha1 89 blockOwnerDeletion: true 90 controller: true 91 kind: SQL 92spec: 93 deletionPolicy: Delete 94 forProvider: 95 region: us-east-1 96 tags: 97 crossplane-kind: internetgateway.ec2.aws.upbound.io 98 crossplane-name: my-db-aws 99 crossplane-providerconfig: default 100 vpcIdSelector: 101 matchControllerRef: true 102 managementPolicy: FullControl 103 providerConfigRef: 104 name: default 105... That one is essentially the same as what we would do with KUTTL. There are no templates or functions simply because that assert is not reused. It is very specific to managed resources for AWS. There's nothing fancy there since there is no need for anything fancy.\nThere's one important note here. Take a look at spec.resourceRefs. It is an array of items and the assert validates that exactly the same items specified here are available in the actual resource in the cluster. They even have to be defined in the same order. No more. No less. It needs to be exactly the same.\nNow, that's perfectly okay in this specific example but, as you'll see later, that can become a nightmare to assert as we start updating resources. For now, remember that there are currently eighteen entries in spec.resourceRefs. That will become important later.\nSpeaking of updates, let's go back to the Test manifest.\n1cat tests/aws/chainsaw-test.yaml The output is as follows (truncated for brevity).\n1apiVersion: chainsaw.kyverno.io/v1alpha1 2kind: Test 3metadata: 4 name: aws 5spec: 6 ... 7 steps: 8 - try: 9 ... 10 - patch: 11 file: ../common/db.yaml 12 - assert: 13 file: ../common/assert-db.yaml 14 ... After the asserts that validated that the resource initially applied and child resources it created are as they are supposed to be, I wanted to update the SQLClaim. With KUTTL, that would mean creating a copy of it, making some modifications, and applying the whole manifest again. That's another example of repetition.\nTo avoid that, I used Chainsaw's patch option, so let's take a look at it.\n1cat tests/common/db.yaml The output is as follows.\n1apiVersion: devopstoolkitseries.com/v1alpha1 2kind: SQLClaim 3metadata: 4 name: my-db 5spec: 6 parameters: 7 databases: 8 - db-01 9 - db-02 10 - db-03 This is very similar to Kustomize overlay mechanism. Instead of specifying the whole SQLClaim again, I'm saying something like \u0026quot;find the SQLClaim named my-db and update it by adding spec.parameters.databases with those three entries. That allows me to avoid repetition, except for the identifiers like apiVersion, kind and metadata.name.\nNow, testing that patch could be complicated, but is actually made relatively simple with Chainsaw, so let's take a look at the assert-db.yaml file that should validate that change.\n1cat tests/common/assert-db.yaml The output is as follows.\n1--- 2apiVersion: devopstoolkitseries.com/v1alpha1 3kind: SQL 4metadata: 5 labels: 6 crossplane.io/claim-name: my-db 7spec: 8 parameters: 9 databases: 10 - db-01 11 - db-02 12 - db-03 13 (resourceRefs[?kind == \u0026#39;Database\u0026#39;]): 14 - apiVersion: postgresql.sql.crossplane.io/v1alpha1 15 name: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler, \u0026#39;db-01\u0026#39;])) 16 - apiVersion: postgresql.sql.crossplane.io/v1alpha1 17 name: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler, \u0026#39;db-02\u0026#39;])) 18 - apiVersion: postgresql.sql.crossplane.io/v1alpha1 19 name: (join(\u0026#39;-\u0026#39;, [\u0026#39;my-db\u0026#39;, $hyperscaler, \u0026#39;db-03\u0026#39;])) 20... The expected outcome of the patch we applied should be the addition of three additional resource spun up by SQL. There were eighteen before and now there should be twenty one. Now, at this stage, I'm only interested in those three so having to list all twenty one sounds like a waste of time. That's what I would have to do with KUTTL, but not with Chainsaw.\nInstead, I can apply filtering. I can say \u0026quot;Get all resourceRefs entries, then filter them so that only those that contain kind set to Database are left. I don't care about the others. I want only those.\u0026quot;\nNow, that I filtered entries of the resourceRefs, I can assert that they are exactly what I expect them to be. There should be my-db-aws-db-01, my-db-aws-db-02, and my-db-aws-db-03. Since the similar pattern should be asserted for Azure and Google as well, that name is assembled using the join function and hyperscaler variable just as we did before.\nThere's one more, out of many features, that I'd like to show.\n1cat tests/aws/chainsaw-test.yaml The output is as follows (truncated for brevity).\n1apiVersion: chainsaw.kyverno.io/v1alpha1 2kind: Test 3metadata: 4 name: aws 5spec: 6 ... 7 steps: 8 - try: 9 ... 10 catch: 11 - get: 12 resource: managed 13 - describe: 14 resource: sqls 15 selector: crossplane.io/claim-namespace=$NAMESPACE Just as most other test frameworks, each step has try, catch, and finally blocks. So far, we explored only try which I used as a mechanism to apply and patch resources, and to assert them. That's the backbone that everyone will be using. But, in some cases, we might want to leverage catch blocks that are executed in case there are failures, and the finally block that is executed always at the end. Personally, I did not find finally useful in my projects since Chainsaw automatically deletes all the resources created in the try block. Nevertheless, if you have some additional cleanup to do, that's when you would do it.\nThe catch block is, at least in my case, very useful. You see, when a test fails, the output Chainsaw provides by default is not always sufficient. I often need more. In this specific case, I need to see all the managed resources running in the cluster as well as the events of the main resource under test.\nThat's why I added two catch statements. The first one is a get that returns all managed resources and the other one is describe that, as the name would suggest, describes the sqls resource with the specific label.\nNow, to be clear, I could accomplish the same with the script that would execute kubectl get managed and kubectl describe commands. The effect would be the same, and the specifics are probably not important for this story. What matters is that there is a mechanism for me to retrieve additional information in case of the failure of one of the asserts and that I can use that information to deduce what I did wrong.\nI only scretched the surface with Kyverno Chainsaw and I'll leave it to you to explore it in more depth, but only after we go through pros and cons.\nKyverno Chainsaw Pros and Cons I must admit something. When I first saw Chainsaw, I was not impressed. I thought that it is moving into a wrong direction. I thought that Kyverno JSON Query Language might not be the best way to express complex logic and that we would be better off writing those tests in a programming language of choice which, in my case, would be Go.\nThat might still be the case. There's still something telling me that I might be better of writing tests as \u0026quot;real\u0026quot; code instead of YAML interpolated with Kyverno JSON Query Language. It does not seem right to convert YAML into anything that it is not already; into anything beyond data structures.\nNevertheless, I do think that Chainsaw is, right now, the best tool to test Kubernetes resources. KUTTL is too limiting and results in too much repetition which I was tempted to avoid through some silly workarounds.\nHence, even before we go through Chainsaw pros and cons, I can safely say use it. It has its faults, just as anything else does, but it is a huge step forward when compared to what we had in the past.\nWith that being said, there are a few negative things, so let's start with those first.\nTODO: Header: Cons; Items: Docs; YAML; Learning curve; No string interpolation\nTo begin with, documentation is not very good. I strugled a lot with it. Docs might be the main reason why my initial impressions of Chainsaw were negative. Fortunately for me, folks behind the project were very helpful and made me \u0026quot;see the light\u0026quot; eventually. I'm sold now. I am committed to Chainsaw when testing my Kuberentes resources, but that's not thanks to documentation but, rather, very approachable maintainers. Now, to be clear, almost all projects have horrible docs in their early stages so I'm not saying that Chainsaw documentation is worse than others but, rather, that it is just as bad as most early stage projects.\nI am not convinced that expressing logic inside YAML is a good idea. If we are in need of loops, conditionals, and other constructs, we are, generally speaking, better of with something like Go, or Python, or Java, or CUE, or any other language you might be familiar with. That being said, Kyverno JSON Query Language is very powerful and it works well. It's not something developed specifically for Chainsaw but, rather, something that has been battle tested with Kyverno itself. Hence, I'm split between this is great and this should not be done with YAML.\nNow that I mentioned Kyverno JSON Query Language, there is also a relatively steep learning curve, at least when testing is concerned. It might take a while until you get a grip on Kyverno JSON Query Language which is the backbone of Chainsaw. If you are already using Kyverno, you should have no trouble jumping into Chainsaw. But if you are not, it might take a while until you feel comfortable.\nFinally, there is no string interpolation. Having to use functions like join instead of simply interpolating strings and variables makes tests harder to write than they should be and even harder to read by those not versed in it.\nThere must be other issues but, at least for me, those are the biggest ones and, frankly, I'm not worried about any of those except the doubt whether to use YAML in the first place. I'll explain later why I'm not worried, just after we go through the good things; though the pros.\nTODO: Header: Pros; Items: Maintainers; Templates, bindings, functions; Output; Try/catch/finaly; Patching; KUTTL compatibility\nThe first and, potentially most important thing about Chainsaw is the openess, enthysiasm, and proactiveness of the maintainers. Now, to be clear, vast majority of the commits are done by two people with occasional commits by a few others so it's not a big project with massive number of maintainers. Never the less, I was impressed to see how proactive they are in reaching to other communities, listening to feedback, adapting the project to the needs of early adopters, and all the other things that projects at such early stage should be doing but are often not. I asked for some features and some of them were already added to the project while others are ongoing. I wanted bindings, I got bindings. I wanted it to be available as a Nix package, and, by the time you watch this, that will probably done. I got burned by not being able to use string interpolations, and I'm sure that will be added soon. That's the reason why I said that I'm not worried about the cons. I'm sure that the issues I discovered will be fixed soon. If you discover something, I'm sure you'll get the same treatment as long as you let the project know what that something is.\nNext, templates, bindings, and functions are awesome. Chainsaw is piggy backing on Kyverno that already has many of those things nailed down, and that's giving Chainsaw a huge boost. Since the project started by Kyverno, they have the know-how that allows them to move fast on top of what they already know. Those alone enabled me to greatly simplify my tests that were previously written for KUTTL and to do things that I could not do before.\nThen there is output that is much easier to read and deduce what's wrong.\nThat output is greatly augmented with additional instructions I can add to the catch block so the mere existence of Try/catch/finaly* blocks is important, especially since it follows a common practice in testing.\nThen there is patching which helps greatly with constant updates of the resources for the sake of creating more robust test scenarios.\nFinally, if you are already using KUTTL, you can switch to Chainsaw without any modifications. Chainsaw can run KUTTL tests as they are. Now, to be clear, you won't get much of the benefits of Chainsaw by only changing the CLI. You will, eventually, have to rewrite some of the tests or start writing new ones in a Chainsaw-friendly way. That's to be expected. But, what matters, at least to KUTTL users, is that you do not need to rewrite all your tests right away. You can just switch to the chainsaw CLI right away without any upfront investment and transition manifests later when you see that you'll gain benefits from that effort.\nAll in all Chainsaw is awesome and I strongly recommend it. It has its quirks but, as far as I know, it is the best tool for testing Kubernetes resources right now.\nThe community is very welcoming and I suggest you get in touch with them if you have a suggestion for improvement.\nThank you for watching. See you in the next one. Cheers.\nDestroy 1task cluster-destroy 2 3git checkout main 4 5exit ","link":"http://localhost:1313/post/chainsaw/","section":"post","tags":null,"title":"Mastering Kubernetes Testing Kyverno Chainsaw!"},{"body":"","link":"http://localhost:1313/archives/","section":"","tags":null,"title":""},{"body":"We want to help you learn the tools and the processes that you should be using and applying in your day-to-day job. We want to help you make decisions. What works well, what doesn't work, why you should choose one tool over the other, and how to get up-to-speed quickly. Which tool works the best for a given task? What should we explore in more depth, and what is a waste of time?\nThis channel has DevOps in the name because we believe that the only way forward is to combine different types of expertise. Ultimately, we need to be able to develop, test, deploy, and operate our systems without friction caused by silos formed around distinct types of expertise. Hence, our focus is on bridging the gap by focusing on the topics that allow developers, operators, and everyone else works together by adopting tools and processes that are relevant today and foster collaboration.\nViktor Farcic \u0026amp; Darin Pope\n","link":"http://localhost:1313/about/","section":"","tags":null,"title":"About DevOps Toolkit"},{"body":"","link":"http://localhost:1313/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"http://localhost:1313/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"http://localhost:1313/tags/","section":"tags","tags":null,"title":"Tags"}]